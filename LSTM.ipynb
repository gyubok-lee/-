{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Dropout, Activation\n",
    "\n",
    "%matplotlib inline\n",
    "# 경고 끄기\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# 경로 지정\n",
    "path = './data/public_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>요일</th>\n",
       "      <th>배추_거래량(kg)</th>\n",
       "      <th>배추_가격(원/kg)</th>\n",
       "      <th>무_거래량(kg)</th>\n",
       "      <th>무_가격(원/kg)</th>\n",
       "      <th>양파_거래량(kg)</th>\n",
       "      <th>양파_가격(원/kg)</th>\n",
       "      <th>건고추_거래량(kg)</th>\n",
       "      <th>건고추_가격(원/kg)</th>\n",
       "      <th>...</th>\n",
       "      <th>청상추_거래량(kg)</th>\n",
       "      <th>청상추_가격(원/kg)</th>\n",
       "      <th>백다다기_거래량(kg)</th>\n",
       "      <th>백다다기_가격(원/kg)</th>\n",
       "      <th>애호박_거래량(kg)</th>\n",
       "      <th>애호박_가격(원/kg)</th>\n",
       "      <th>캠벨얼리_거래량(kg)</th>\n",
       "      <th>캠벨얼리_가격(원/kg)</th>\n",
       "      <th>샤인마스캇_거래량(kg)</th>\n",
       "      <th>샤인마스캇_가격(원/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>금요일</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>토요일</td>\n",
       "      <td>80860.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>80272.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>122787.5</td>\n",
       "      <td>1281.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5125.0</td>\n",
       "      <td>9235.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>2109.0</td>\n",
       "      <td>19159.0</td>\n",
       "      <td>2414.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>일요일</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>월요일</td>\n",
       "      <td>1422742.5</td>\n",
       "      <td>478.0</td>\n",
       "      <td>1699653.7</td>\n",
       "      <td>382.0</td>\n",
       "      <td>2315079.0</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>4464.0</td>\n",
       "      <td>...</td>\n",
       "      <td>38525.5</td>\n",
       "      <td>7631.0</td>\n",
       "      <td>500702.0</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>620539.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>2703.8</td>\n",
       "      <td>3885.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>화요일</td>\n",
       "      <td>1167241.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>1423482.3</td>\n",
       "      <td>422.0</td>\n",
       "      <td>2092960.1</td>\n",
       "      <td>1213.0</td>\n",
       "      <td>1112.6</td>\n",
       "      <td>4342.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32615.0</td>\n",
       "      <td>6926.0</td>\n",
       "      <td>147638.0</td>\n",
       "      <td>2268.0</td>\n",
       "      <td>231958.0</td>\n",
       "      <td>2178.0</td>\n",
       "      <td>8810.0</td>\n",
       "      <td>2853.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   요일  배추_거래량(kg)  배추_가격(원/kg)  무_거래량(kg)  무_가격(원/kg)  \\\n",
       "0  2016-01-01  금요일         0.0          0.0        0.0         0.0   \n",
       "1  2016-01-02  토요일     80860.0        329.0    80272.0       360.0   \n",
       "2  2016-01-03  일요일         0.0          0.0        0.0         0.0   \n",
       "3  2016-01-04  월요일   1422742.5        478.0  1699653.7       382.0   \n",
       "4  2016-01-05  화요일   1167241.0        442.0  1423482.3       422.0   \n",
       "\n",
       "   양파_거래량(kg)  양파_가격(원/kg)  건고추_거래량(kg)  건고추_가격(원/kg)  ...  청상추_거래량(kg)  \\\n",
       "0         0.0          0.0          0.0           0.0  ...          0.0   \n",
       "1    122787.5       1281.0          3.0       11000.0  ...       5125.0   \n",
       "2         0.0          0.0          0.0           0.0  ...          0.0   \n",
       "3   2315079.0       1235.0        699.0        4464.0  ...      38525.5   \n",
       "4   2092960.1       1213.0       1112.6        4342.0  ...      32615.0   \n",
       "\n",
       "   청상추_가격(원/kg)  백다다기_거래량(kg)  백다다기_가격(원/kg)  애호박_거래량(kg)  애호박_가격(원/kg)  \\\n",
       "0           0.0           0.0            0.0          0.0           0.0   \n",
       "1        9235.0         434.0         2109.0      19159.0        2414.0   \n",
       "2           0.0           0.0            0.0          0.0           0.0   \n",
       "3        7631.0      500702.0         2046.0     620539.0        2018.0   \n",
       "4        6926.0      147638.0         2268.0     231958.0        2178.0   \n",
       "\n",
       "   캠벨얼리_거래량(kg)  캠벨얼리_가격(원/kg)  샤인마스캇_거래량(kg)  샤인마스캇_가격(원/kg)  \n",
       "0           0.0            0.0            0.0             0.0  \n",
       "1         880.0         2014.0            0.0             0.0  \n",
       "2           0.0            0.0            0.0             0.0  \n",
       "3        2703.8         3885.0            0.0             0.0  \n",
       "4        8810.0         2853.0            0.0             0.0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(path +'train.csv')\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(temp_df, pum) :\n",
    "    \n",
    "    # 거래가 없는 날짜 제외\n",
    "    temp_df = temp_df[(temp_df[f'{pum}_가격(원/kg)'] > 0) & (temp_df[f'{pum}_거래량(kg)'] > 0)].reset_index(drop = True)\n",
    "    \n",
    "    # 이동평균(5일,10일,20일) 추가 \n",
    "    temp_df['p_ma5'] = temp_df[f'{pum}_가격(원/kg)'].rolling(window=5).mean().fillna(0)\n",
    "    temp_df['p_ma10'] = temp_df[f'{pum}_가격(원/kg)'].rolling(window=10).mean().fillna(0)\n",
    "    temp_df['p_ma20'] = temp_df[f'{pum}_가격(원/kg)'].rolling(window=20).mean().fillna(0)\n",
    "    \n",
    "    temp_df['q_ma5'] = temp_df[f'{pum}_거래량(kg)'].rolling(window=5).mean().fillna(0)\n",
    "    temp_df['q_ma10'] = temp_df[f'{pum}_거래량(kg)'].rolling(window=10).mean().fillna(0)\n",
    "    temp_df['q_ma20'] = temp_df[f'{pum}_거래량(kg)'].rolling(window=20).mean().fillna(0)\n",
    "    \n",
    "     # p_lag, q_lag 추가\n",
    "    for lag in range(1,6) :\n",
    "        temp_df[f'p_lag_{lag}'] = -1\n",
    "        temp_df[f'q_lag_{lag}'] = -1\n",
    "        for index in range(lag, len(temp_df)) :\n",
    "            temp_df.loc[index, f'p_lag_{lag}'] = temp_df[f'{pum}_가격(원/kg)'][index-lag] #1일전, 2일전, ... 가격을 feature로 추가\n",
    "            temp_df.loc[index, f'q_lag_{lag}'] = temp_df[f'{pum}_거래량(kg)'][index-lag] #1일전, 2일전, ... 거래량을 feature로 추가    \n",
    "    \n",
    "    # 예측 대상(1w,2w,4w) 추가\n",
    "    for week in ['1_week','2_week','4_week'] :\n",
    "        temp_df[week] = 0\n",
    "        n_week = int(week[0])\n",
    "        for index in range(len(temp_df)) :\n",
    "            try : temp_df[week][index] = temp_df[f'{pum}_가격(원/kg)'][index+7*n_week]\n",
    "            except : continue\n",
    "\n",
    "    # 불필요한 column 제거        \n",
    "    temp_df = temp_df.drop(['date'], axis=1)\n",
    "    \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing 함수 예시\n",
    "pum = '배추'\n",
    "temp_df = train[['date',f'{pum}_거래량(kg)', f'{pum}_가격(원/kg)']]\n",
    "pp = preprocessing(temp_df, pum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>배추_거래량(kg)</th>\n",
       "      <th>배추_가격(원/kg)</th>\n",
       "      <th>p_ma5</th>\n",
       "      <th>p_ma10</th>\n",
       "      <th>p_ma20</th>\n",
       "      <th>q_ma5</th>\n",
       "      <th>q_ma10</th>\n",
       "      <th>q_ma20</th>\n",
       "      <th>p_lag_1</th>\n",
       "      <th>q_lag_1</th>\n",
       "      <th>...</th>\n",
       "      <th>q_lag_2</th>\n",
       "      <th>p_lag_3</th>\n",
       "      <th>q_lag_3</th>\n",
       "      <th>p_lag_4</th>\n",
       "      <th>q_lag_4</th>\n",
       "      <th>p_lag_5</th>\n",
       "      <th>q_lag_5</th>\n",
       "      <th>1_week</th>\n",
       "      <th>2_week</th>\n",
       "      <th>4_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>1020033.2</td>\n",
       "      <td>1561.0</td>\n",
       "      <td>1457.8</td>\n",
       "      <td>1385.6</td>\n",
       "      <td>1244.90</td>\n",
       "      <td>1007047.90</td>\n",
       "      <td>1013679.31</td>\n",
       "      <td>1040226.370</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>1124756.3</td>\n",
       "      <td>...</td>\n",
       "      <td>1119462.3</td>\n",
       "      <td>1538.0</td>\n",
       "      <td>1081785.4</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>689202.3</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>965833.7</td>\n",
       "      <td>1358</td>\n",
       "      <td>1748</td>\n",
       "      <td>3066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>763266.0</td>\n",
       "      <td>1564.0</td>\n",
       "      <td>1490.4</td>\n",
       "      <td>1393.3</td>\n",
       "      <td>1272.55</td>\n",
       "      <td>1021860.64</td>\n",
       "      <td>989366.01</td>\n",
       "      <td>1021159.440</td>\n",
       "      <td>1561.0</td>\n",
       "      <td>1020033.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1124756.3</td>\n",
       "      <td>1416.0</td>\n",
       "      <td>1119462.3</td>\n",
       "      <td>1538.0</td>\n",
       "      <td>1081785.4</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>689202.3</td>\n",
       "      <td>1329</td>\n",
       "      <td>2042</td>\n",
       "      <td>1867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>760499.0</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>1478.0</td>\n",
       "      <td>1410.4</td>\n",
       "      <td>1299.40</td>\n",
       "      <td>957603.36</td>\n",
       "      <td>958628.44</td>\n",
       "      <td>1002757.870</td>\n",
       "      <td>1564.0</td>\n",
       "      <td>763266.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1020033.2</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>1124756.3</td>\n",
       "      <td>1416.0</td>\n",
       "      <td>1119462.3</td>\n",
       "      <td>1538.0</td>\n",
       "      <td>1081785.4</td>\n",
       "      <td>1614</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>1441152.8</td>\n",
       "      <td>1133.0</td>\n",
       "      <td>1421.4</td>\n",
       "      <td>1405.5</td>\n",
       "      <td>1300.80</td>\n",
       "      <td>1021941.46</td>\n",
       "      <td>992168.42</td>\n",
       "      <td>1030357.390</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>760499.0</td>\n",
       "      <td>...</td>\n",
       "      <td>763266.0</td>\n",
       "      <td>1561.0</td>\n",
       "      <td>1020033.2</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>1124756.3</td>\n",
       "      <td>1416.0</td>\n",
       "      <td>1119462.3</td>\n",
       "      <td>1994</td>\n",
       "      <td>1939</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>1279591.6</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>1365.4</td>\n",
       "      <td>1384.5</td>\n",
       "      <td>1302.45</td>\n",
       "      <td>1052908.52</td>\n",
       "      <td>1024558.26</td>\n",
       "      <td>1044450.160</td>\n",
       "      <td>1133.0</td>\n",
       "      <td>1441152.8</td>\n",
       "      <td>...</td>\n",
       "      <td>760499.0</td>\n",
       "      <td>1564.0</td>\n",
       "      <td>763266.0</td>\n",
       "      <td>1561.0</td>\n",
       "      <td>1020033.2</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>1124756.3</td>\n",
       "      <td>1585</td>\n",
       "      <td>1983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1144746.8</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1283.2</td>\n",
       "      <td>1370.5</td>\n",
       "      <td>1309.35</td>\n",
       "      <td>1077851.24</td>\n",
       "      <td>1042449.57</td>\n",
       "      <td>1036374.480</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>1279591.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1441152.8</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>760499.0</td>\n",
       "      <td>1564.0</td>\n",
       "      <td>763266.0</td>\n",
       "      <td>1561.0</td>\n",
       "      <td>1020033.2</td>\n",
       "      <td>1542</td>\n",
       "      <td>1839</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>895628.0</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>1259.4</td>\n",
       "      <td>1374.9</td>\n",
       "      <td>1325.25</td>\n",
       "      <td>1104323.64</td>\n",
       "      <td>1063092.14</td>\n",
       "      <td>1025506.785</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1144746.8</td>\n",
       "      <td>...</td>\n",
       "      <td>1279591.6</td>\n",
       "      <td>1133.0</td>\n",
       "      <td>1441152.8</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>760499.0</td>\n",
       "      <td>1564.0</td>\n",
       "      <td>763266.0</td>\n",
       "      <td>1576</td>\n",
       "      <td>1812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>698187.5</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>1235.8</td>\n",
       "      <td>1356.9</td>\n",
       "      <td>1338.10</td>\n",
       "      <td>1091861.34</td>\n",
       "      <td>1024732.35</td>\n",
       "      <td>1019214.040</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>895628.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1144746.8</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>1279591.6</td>\n",
       "      <td>1133.0</td>\n",
       "      <td>1441152.8</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>760499.0</td>\n",
       "      <td>1748</td>\n",
       "      <td>2925</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1104424.8</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>1275.0</td>\n",
       "      <td>1348.2</td>\n",
       "      <td>1344.80</td>\n",
       "      <td>1024515.74</td>\n",
       "      <td>1023228.60</td>\n",
       "      <td>1015096.780</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>698187.5</td>\n",
       "      <td>...</td>\n",
       "      <td>895628.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1144746.8</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>1279591.6</td>\n",
       "      <td>1133.0</td>\n",
       "      <td>1441152.8</td>\n",
       "      <td>2042</td>\n",
       "      <td>1813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>975020.2</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>1379.2</td>\n",
       "      <td>1372.3</td>\n",
       "      <td>1364.85</td>\n",
       "      <td>963601.46</td>\n",
       "      <td>1008254.99</td>\n",
       "      <td>1006309.110</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>1104424.8</td>\n",
       "      <td>...</td>\n",
       "      <td>698187.5</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>895628.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1144746.8</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>1279591.6</td>\n",
       "      <td>2017</td>\n",
       "      <td>1838</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>540843.7</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>1548.0</td>\n",
       "      <td>1415.6</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>842820.84</td>\n",
       "      <td>960336.04</td>\n",
       "      <td>987007.675</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>975020.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1104424.8</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>698187.5</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>895628.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1144746.8</td>\n",
       "      <td>1939</td>\n",
       "      <td>1807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>1181782.2</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>1576.0</td>\n",
       "      <td>1417.7</td>\n",
       "      <td>1405.50</td>\n",
       "      <td>900051.68</td>\n",
       "      <td>1002187.66</td>\n",
       "      <td>995776.835</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>540843.7</td>\n",
       "      <td>...</td>\n",
       "      <td>975020.2</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>1104424.8</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>698187.5</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>895628.0</td>\n",
       "      <td>1983</td>\n",
       "      <td>1839</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>1302290.8</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>1612.8</td>\n",
       "      <td>1424.3</td>\n",
       "      <td>1417.35</td>\n",
       "      <td>1020872.34</td>\n",
       "      <td>1056366.84</td>\n",
       "      <td>1007497.640</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>1181782.2</td>\n",
       "      <td>...</td>\n",
       "      <td>540843.7</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>975020.2</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>1104424.8</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>698187.5</td>\n",
       "      <td>1839</td>\n",
       "      <td>1789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>1156811.6</td>\n",
       "      <td>1576.0</td>\n",
       "      <td>1662.2</td>\n",
       "      <td>1468.6</td>\n",
       "      <td>1437.05</td>\n",
       "      <td>1031349.70</td>\n",
       "      <td>1027932.72</td>\n",
       "      <td>1010050.570</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>1302290.8</td>\n",
       "      <td>...</td>\n",
       "      <td>1181782.2</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>540843.7</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>975020.2</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>1104424.8</td>\n",
       "      <td>1812</td>\n",
       "      <td>1760</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>968110.0</td>\n",
       "      <td>1748.0</td>\n",
       "      <td>1689.0</td>\n",
       "      <td>1534.1</td>\n",
       "      <td>1459.30</td>\n",
       "      <td>1029967.66</td>\n",
       "      <td>996784.56</td>\n",
       "      <td>1010671.410</td>\n",
       "      <td>1576.0</td>\n",
       "      <td>1156811.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1302290.8</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>1181782.2</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>540843.7</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>975020.2</td>\n",
       "      <td>2925</td>\n",
       "      <td>3066</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>1221538.6</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>1698.6</td>\n",
       "      <td>1623.3</td>\n",
       "      <td>1496.90</td>\n",
       "      <td>1166106.64</td>\n",
       "      <td>1004463.74</td>\n",
       "      <td>1023456.655</td>\n",
       "      <td>1748.0</td>\n",
       "      <td>968110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1156811.6</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>1302290.8</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>1181782.2</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>540843.7</td>\n",
       "      <td>1813</td>\n",
       "      <td>1867</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>1341387.6</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1785.0</td>\n",
       "      <td>1680.5</td>\n",
       "      <td>1527.70</td>\n",
       "      <td>1198027.72</td>\n",
       "      <td>1049039.70</td>\n",
       "      <td>1056065.920</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>1221538.6</td>\n",
       "      <td>...</td>\n",
       "      <td>968110.0</td>\n",
       "      <td>1576.0</td>\n",
       "      <td>1156811.6</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>1302290.8</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>1181782.2</td>\n",
       "      <td>1838</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>1532777.2</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1864.4</td>\n",
       "      <td>1738.6</td>\n",
       "      <td>1547.75</td>\n",
       "      <td>1244125.00</td>\n",
       "      <td>1132498.67</td>\n",
       "      <td>1078615.510</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1341387.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1221538.6</td>\n",
       "      <td>1748.0</td>\n",
       "      <td>968110.0</td>\n",
       "      <td>1576.0</td>\n",
       "      <td>1156811.6</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>1302290.8</td>\n",
       "      <td>1807</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>1459130.1</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>1945.8</td>\n",
       "      <td>1804.0</td>\n",
       "      <td>1576.10</td>\n",
       "      <td>1304588.70</td>\n",
       "      <td>1167969.20</td>\n",
       "      <td>1095598.900</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1532777.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1341387.6</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>1221538.6</td>\n",
       "      <td>1748.0</td>\n",
       "      <td>968110.0</td>\n",
       "      <td>1576.0</td>\n",
       "      <td>1156811.6</td>\n",
       "      <td>1839</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>1624514.7</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1826.5</td>\n",
       "      <td>1599.40</td>\n",
       "      <td>1435869.64</td>\n",
       "      <td>1232918.65</td>\n",
       "      <td>1120586.820</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>1459130.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1532777.2</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1341387.6</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>1221538.6</td>\n",
       "      <td>1748.0</td>\n",
       "      <td>968110.0</td>\n",
       "      <td>1789</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>1346091.4</td>\n",
       "      <td>1812.0</td>\n",
       "      <td>1918.0</td>\n",
       "      <td>1808.3</td>\n",
       "      <td>1611.95</td>\n",
       "      <td>1460780.20</td>\n",
       "      <td>1313443.42</td>\n",
       "      <td>1136889.730</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>1624514.7</td>\n",
       "      <td>...</td>\n",
       "      <td>1459130.1</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1532777.2</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1341387.6</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>1221538.6</td>\n",
       "      <td>1760</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>1959.0</td>\n",
       "      <td>2925.0</td>\n",
       "      <td>2099.6</td>\n",
       "      <td>1942.3</td>\n",
       "      <td>1680.00</td>\n",
       "      <td>1192894.48</td>\n",
       "      <td>1195461.10</td>\n",
       "      <td>1098824.380</td>\n",
       "      <td>1812.0</td>\n",
       "      <td>1346091.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1624514.7</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>1459130.1</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1532777.2</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1341387.6</td>\n",
       "      <td>3066</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>2046286.3</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>2074.4</td>\n",
       "      <td>1969.4</td>\n",
       "      <td>1696.85</td>\n",
       "      <td>1295596.30</td>\n",
       "      <td>1269860.65</td>\n",
       "      <td>1163113.745</td>\n",
       "      <td>2925.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1346091.4</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>1624514.7</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>1459130.1</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1532777.2</td>\n",
       "      <td>1867</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>1757465.6</td>\n",
       "      <td>1838.0</td>\n",
       "      <td>2045.4</td>\n",
       "      <td>1995.6</td>\n",
       "      <td>1732.10</td>\n",
       "      <td>1355263.40</td>\n",
       "      <td>1329926.05</td>\n",
       "      <td>1178929.385</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>2046286.3</td>\n",
       "      <td>...</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>1812.0</td>\n",
       "      <td>1346091.4</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>1624514.7</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>1459130.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>2007471.3</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>2039.0</td>\n",
       "      <td>2001.5</td>\n",
       "      <td>1767.80</td>\n",
       "      <td>1431854.72</td>\n",
       "      <td>1433862.18</td>\n",
       "      <td>1215323.370</td>\n",
       "      <td>1838.0</td>\n",
       "      <td>1757465.6</td>\n",
       "      <td>...</td>\n",
       "      <td>2046286.3</td>\n",
       "      <td>2925.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>1812.0</td>\n",
       "      <td>1346091.4</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>1624514.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>1856965.0</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>2044.4</td>\n",
       "      <td>1981.2</td>\n",
       "      <td>1802.25</td>\n",
       "      <td>1534029.44</td>\n",
       "      <td>1497404.82</td>\n",
       "      <td>1250934.280</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>2007471.3</td>\n",
       "      <td>...</td>\n",
       "      <td>1757465.6</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>2046286.3</td>\n",
       "      <td>2925.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>1812.0</td>\n",
       "      <td>1346091.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>1880095.5</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>1817.2</td>\n",
       "      <td>1958.4</td>\n",
       "      <td>1819.45</td>\n",
       "      <td>1909656.74</td>\n",
       "      <td>1551275.61</td>\n",
       "      <td>1300157.655</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>1856965.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2007471.3</td>\n",
       "      <td>1838.0</td>\n",
       "      <td>1757465.6</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>2046286.3</td>\n",
       "      <td>2925.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477</th>\n",
       "      <td>1661090.9</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>1806.6</td>\n",
       "      <td>1940.5</td>\n",
       "      <td>1839.55</td>\n",
       "      <td>1832617.66</td>\n",
       "      <td>1564106.98</td>\n",
       "      <td>1348302.825</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>1880095.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1856965.0</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>2007471.3</td>\n",
       "      <td>1838.0</td>\n",
       "      <td>1757465.6</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>2046286.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>25396.0</td>\n",
       "      <td>3066.0</td>\n",
       "      <td>2052.2</td>\n",
       "      <td>2048.8</td>\n",
       "      <td>1926.40</td>\n",
       "      <td>1486203.74</td>\n",
       "      <td>1420733.57</td>\n",
       "      <td>1294351.385</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>1661090.9</td>\n",
       "      <td>...</td>\n",
       "      <td>1880095.5</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>1856965.0</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>2007471.3</td>\n",
       "      <td>1838.0</td>\n",
       "      <td>1757465.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>2405051.9</td>\n",
       "      <td>1867.0</td>\n",
       "      <td>2064.2</td>\n",
       "      <td>2051.6</td>\n",
       "      <td>1939.05</td>\n",
       "      <td>1565719.86</td>\n",
       "      <td>1498787.29</td>\n",
       "      <td>1365852.970</td>\n",
       "      <td>3066.0</td>\n",
       "      <td>25396.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1661090.9</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>1880095.5</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>1856965.0</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>2007471.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      배추_거래량(kg)  배추_가격(원/kg)   p_ma5  p_ma10   p_ma20       q_ma5  \\\n",
       "1450   1020033.2       1561.0  1457.8  1385.6  1244.90  1007047.90   \n",
       "1451    763266.0       1564.0  1490.4  1393.3  1272.55  1021860.64   \n",
       "1452    760499.0       1476.0  1478.0  1410.4  1299.40   957603.36   \n",
       "1453   1441152.8       1133.0  1421.4  1405.5  1300.80  1021941.46   \n",
       "1454   1279591.6       1093.0  1365.4  1384.5  1302.45  1052908.52   \n",
       "1455   1144746.8       1150.0  1283.2  1370.5  1309.35  1077851.24   \n",
       "1456    895628.0       1445.0  1259.4  1374.9  1325.25  1104323.64   \n",
       "1457    698187.5       1358.0  1235.8  1356.9  1338.10  1091861.34   \n",
       "1458   1104424.8       1329.0  1275.0  1348.2  1344.80  1024515.74   \n",
       "1459    975020.2       1614.0  1379.2  1372.3  1364.85   963601.46   \n",
       "1460    540843.7       1994.0  1548.0  1415.6  1400.60   842820.84   \n",
       "1461   1181782.2       1585.0  1576.0  1417.7  1405.50   900051.68   \n",
       "1462   1302290.8       1542.0  1612.8  1424.3  1417.35  1020872.34   \n",
       "1463   1156811.6       1576.0  1662.2  1468.6  1437.05  1031349.70   \n",
       "1464    968110.0       1748.0  1689.0  1534.1  1459.30  1029967.66   \n",
       "1465   1221538.6       2042.0  1698.6  1623.3  1496.90  1166106.64   \n",
       "1466   1341387.6       2017.0  1785.0  1680.5  1527.70  1198027.72   \n",
       "1467   1532777.2       1939.0  1864.4  1738.6  1547.75  1244125.00   \n",
       "1468   1459130.1       1983.0  1945.8  1804.0  1576.10  1304588.70   \n",
       "1469   1624514.7       1839.0  1964.0  1826.5  1599.40  1435869.64   \n",
       "1470   1346091.4       1812.0  1918.0  1808.3  1611.95  1460780.20   \n",
       "1471      1959.0       2925.0  2099.6  1942.3  1680.00  1192894.48   \n",
       "1472   2046286.3       1813.0  2074.4  1969.4  1696.85  1295596.30   \n",
       "1473   1757465.6       1838.0  2045.4  1995.6  1732.10  1355263.40   \n",
       "1474   2007471.3       1807.0  2039.0  2001.5  1767.80  1431854.72   \n",
       "1475   1856965.0       1839.0  2044.4  1981.2  1802.25  1534029.44   \n",
       "1476   1880095.5       1789.0  1817.2  1958.4  1819.45  1909656.74   \n",
       "1477   1661090.9       1760.0  1806.6  1940.5  1839.55  1832617.66   \n",
       "1478     25396.0       3066.0  2052.2  2048.8  1926.40  1486203.74   \n",
       "1479   2405051.9       1867.0  2064.2  2051.6  1939.05  1565719.86   \n",
       "\n",
       "          q_ma10       q_ma20  p_lag_1    q_lag_1  ...    q_lag_2  p_lag_3  \\\n",
       "1450  1013679.31  1040226.370   1373.0  1124756.3  ...  1119462.3   1538.0   \n",
       "1451   989366.01  1021159.440   1561.0  1020033.2  ...  1124756.3   1416.0   \n",
       "1452   958628.44  1002757.870   1564.0   763266.0  ...  1020033.2   1373.0   \n",
       "1453   992168.42  1030357.390   1476.0   760499.0  ...   763266.0   1561.0   \n",
       "1454  1024558.26  1044450.160   1133.0  1441152.8  ...   760499.0   1564.0   \n",
       "1455  1042449.57  1036374.480   1093.0  1279591.6  ...  1441152.8   1476.0   \n",
       "1456  1063092.14  1025506.785   1150.0  1144746.8  ...  1279591.6   1133.0   \n",
       "1457  1024732.35  1019214.040   1445.0   895628.0  ...  1144746.8   1093.0   \n",
       "1458  1023228.60  1015096.780   1358.0   698187.5  ...   895628.0   1150.0   \n",
       "1459  1008254.99  1006309.110   1329.0  1104424.8  ...   698187.5   1445.0   \n",
       "1460   960336.04   987007.675   1614.0   975020.2  ...  1104424.8   1358.0   \n",
       "1461  1002187.66   995776.835   1994.0   540843.7  ...   975020.2   1329.0   \n",
       "1462  1056366.84  1007497.640   1585.0  1181782.2  ...   540843.7   1614.0   \n",
       "1463  1027932.72  1010050.570   1542.0  1302290.8  ...  1181782.2   1994.0   \n",
       "1464   996784.56  1010671.410   1576.0  1156811.6  ...  1302290.8   1585.0   \n",
       "1465  1004463.74  1023456.655   1748.0   968110.0  ...  1156811.6   1542.0   \n",
       "1466  1049039.70  1056065.920   2042.0  1221538.6  ...   968110.0   1576.0   \n",
       "1467  1132498.67  1078615.510   2017.0  1341387.6  ...  1221538.6   1748.0   \n",
       "1468  1167969.20  1095598.900   1939.0  1532777.2  ...  1341387.6   2042.0   \n",
       "1469  1232918.65  1120586.820   1983.0  1459130.1  ...  1532777.2   2017.0   \n",
       "1470  1313443.42  1136889.730   1839.0  1624514.7  ...  1459130.1   1939.0   \n",
       "1471  1195461.10  1098824.380   1812.0  1346091.4  ...  1624514.7   1983.0   \n",
       "1472  1269860.65  1163113.745   2925.0     1959.0  ...  1346091.4   1839.0   \n",
       "1473  1329926.05  1178929.385   1813.0  2046286.3  ...     1959.0   1812.0   \n",
       "1474  1433862.18  1215323.370   1838.0  1757465.6  ...  2046286.3   2925.0   \n",
       "1475  1497404.82  1250934.280   1807.0  2007471.3  ...  1757465.6   1813.0   \n",
       "1476  1551275.61  1300157.655   1839.0  1856965.0  ...  2007471.3   1838.0   \n",
       "1477  1564106.98  1348302.825   1789.0  1880095.5  ...  1856965.0   1807.0   \n",
       "1478  1420733.57  1294351.385   1760.0  1661090.9  ...  1880095.5   1839.0   \n",
       "1479  1498787.29  1365852.970   3066.0    25396.0  ...  1661090.9   1789.0   \n",
       "\n",
       "        q_lag_3  p_lag_4    q_lag_4  p_lag_5    q_lag_5  1_week  2_week  \\\n",
       "1450  1081785.4   1401.0   689202.3   1290.0   965833.7    1358    1748   \n",
       "1451  1119462.3   1538.0  1081785.4   1401.0   689202.3    1329    2042   \n",
       "1452  1124756.3   1416.0  1119462.3   1538.0  1081785.4    1614    2017   \n",
       "1453  1020033.2   1373.0  1124756.3   1416.0  1119462.3    1994    1939   \n",
       "1454   763266.0   1561.0  1020033.2   1373.0  1124756.3    1585    1983   \n",
       "1455   760499.0   1564.0   763266.0   1561.0  1020033.2    1542    1839   \n",
       "1456  1441152.8   1476.0   760499.0   1564.0   763266.0    1576    1812   \n",
       "1457  1279591.6   1133.0  1441152.8   1476.0   760499.0    1748    2925   \n",
       "1458  1144746.8   1093.0  1279591.6   1133.0  1441152.8    2042    1813   \n",
       "1459   895628.0   1150.0  1144746.8   1093.0  1279591.6    2017    1838   \n",
       "1460   698187.5   1445.0   895628.0   1150.0  1144746.8    1939    1807   \n",
       "1461  1104424.8   1358.0   698187.5   1445.0   895628.0    1983    1839   \n",
       "1462   975020.2   1329.0  1104424.8   1358.0   698187.5    1839    1789   \n",
       "1463   540843.7   1614.0   975020.2   1329.0  1104424.8    1812    1760   \n",
       "1464  1181782.2   1994.0   540843.7   1614.0   975020.2    2925    3066   \n",
       "1465  1302290.8   1585.0  1181782.2   1994.0   540843.7    1813    1867   \n",
       "1466  1156811.6   1542.0  1302290.8   1585.0  1181782.2    1838       0   \n",
       "1467   968110.0   1576.0  1156811.6   1542.0  1302290.8    1807       0   \n",
       "1468  1221538.6   1748.0   968110.0   1576.0  1156811.6    1839       0   \n",
       "1469  1341387.6   2042.0  1221538.6   1748.0   968110.0    1789       0   \n",
       "1470  1532777.2   2017.0  1341387.6   2042.0  1221538.6    1760       0   \n",
       "1471  1459130.1   1939.0  1532777.2   2017.0  1341387.6    3066       0   \n",
       "1472  1624514.7   1983.0  1459130.1   1939.0  1532777.2    1867       0   \n",
       "1473  1346091.4   1839.0  1624514.7   1983.0  1459130.1       0       0   \n",
       "1474     1959.0   1812.0  1346091.4   1839.0  1624514.7       0       0   \n",
       "1475  2046286.3   2925.0     1959.0   1812.0  1346091.4       0       0   \n",
       "1476  1757465.6   1813.0  2046286.3   2925.0     1959.0       0       0   \n",
       "1477  2007471.3   1838.0  1757465.6   1813.0  2046286.3       0       0   \n",
       "1478  1856965.0   1807.0  2007471.3   1838.0  1757465.6       0       0   \n",
       "1479  1880095.5   1839.0  1856965.0   1807.0  2007471.3       0       0   \n",
       "\n",
       "      4_week  \n",
       "1450    3066  \n",
       "1451    1867  \n",
       "1452       0  \n",
       "1453       0  \n",
       "1454       0  \n",
       "1455       0  \n",
       "1456       0  \n",
       "1457       0  \n",
       "1458       0  \n",
       "1459       0  \n",
       "1460       0  \n",
       "1461       0  \n",
       "1462       0  \n",
       "1463       0  \n",
       "1464       0  \n",
       "1465       0  \n",
       "1466       0  \n",
       "1467       0  \n",
       "1468       0  \n",
       "1469       0  \n",
       "1470       0  \n",
       "1471       0  \n",
       "1472       0  \n",
       "1473       0  \n",
       "1474       0  \n",
       "1475       0  \n",
       "1476       0  \n",
       "1477       0  \n",
       "1478       0  \n",
       "1479       0  \n",
       "\n",
       "[30 rows x 21 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmae(week_answer, week_submission):\n",
    "    answer = week_answer#.to_numpy()\n",
    "    target_idx = np.where(answer!=0)\n",
    "    true = answer[target_idx]\n",
    "    pred = week_submission[target_idx]\n",
    "    score = np.mean(np.abs(true-pred)/true)\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def at_nmae(pred, dataset):\n",
    "    y_true = dataset.get_label()\n",
    "    week_1_answer = y_true[0::3]\n",
    "    week_2_answer = y_true[1::3]\n",
    "    week_4_answer = y_true[2::3]\n",
    "    \n",
    "    week_1_submission = pred[0::3]\n",
    "    week_2_submission = pred[1::3]\n",
    "    week_4_submission = pred[2::3]\n",
    "    \n",
    "    score1 = nmae(week_1_answer, week_1_submission)\n",
    "    score2 = nmae(week_2_answer, week_2_submission)\n",
    "    score4 = nmae(week_4_answer, week_4_submission)\n",
    "    \n",
    "    score = (score1+score2+score4)/3\n",
    "    \n",
    "    return 'score', score, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pum = [\n",
    "    '배추', '무', '양파', '건고추','마늘',\n",
    "    '대파', '얼갈이배추', '양배추', '깻잎',\n",
    "    '시금치', '미나리', '당근',\n",
    "    '파프리카', '새송이', '팽이버섯', '토마토',\n",
    "]\n",
    "\n",
    "unique_kind = [\n",
    "    '청상추', '백다다기', '애호박', '캠벨얼리', '샤인마스캇'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data, label, window_size=4):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))\n",
    "    return np.array(feature_list), np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1142, 4, 18) (286, 4, 18) (1142,) (286,)\n",
      "Train on 1142 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1142/1142 [==============================] - 8s 7ms/sample - loss: 226196.8857 - val_loss: 45953.9495\n",
      "Epoch 2/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 77582.2993 - val_loss: 28985.1982\n",
      "Epoch 3/10\n",
      "1142/1142 [==============================] - 4s 3ms/sample - loss: 63219.1403 - val_loss: 36387.5153\n",
      "Epoch 4/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 57861.4871 - val_loss: 21713.0989\n",
      "Epoch 5/10\n",
      "1142/1142 [==============================] - 4s 3ms/sample - loss: 52632.0049 - val_loss: 20931.1672\n",
      "Epoch 6/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 51777.7130 - val_loss: 26861.5255\n",
      "Epoch 7/10\n",
      "1142/1142 [==============================] - 4s 3ms/sample - loss: 49255.1114 - val_loss: 23292.3458\n",
      "Epoch 8/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 48912.6222 - val_loss: 22332.6439\n",
      "Epoch 9/10\n",
      "1142/1142 [==============================] - 4s 3ms/sample - loss: 50207.4656 - val_loss: 19714.7988\n",
      "Epoch 10/10\n",
      "1142/1142 [==============================] - 4s 3ms/sample - loss: 46506.4413 - val_loss: 24411.5496\n",
      "(1142, 4, 18) (286, 4, 18) (1142,) (286,)\n",
      "Train on 1142 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1142/1142 [==============================] - 7s 6ms/sample - loss: 241142.8655 - val_loss: 57031.0403\n",
      "Epoch 2/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 71526.7637 - val_loss: 51555.2039\n",
      "Epoch 3/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 54549.2486 - val_loss: 49210.0737\n",
      "Epoch 4/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 46011.4138 - val_loss: 47182.6874\n",
      "Epoch 5/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 41806.1720 - val_loss: 54323.8801\n",
      "Epoch 6/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 37135.8007 - val_loss: 47319.3924\n",
      "Epoch 7/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 36043.8558 - val_loss: 45403.6707\n",
      "Epoch 8/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 35775.2765 - val_loss: 54387.9574\n",
      "Epoch 9/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 35134.5171 - val_loss: 51122.7593\n",
      "Epoch 10/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 36131.2570 - val_loss: 47745.3299\n",
      "(1142, 4, 18) (286, 4, 18) (1142,) (286,)\n",
      "Train on 1142 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1142/1142 [==============================] - 8s 7ms/sample - loss: 231859.6540 - val_loss: 157644.1850\n",
      "Epoch 2/10\n",
      "1142/1142 [==============================] - 4s 3ms/sample - loss: 92437.8311 - val_loss: 171475.1841\n",
      "Epoch 3/10\n",
      "1142/1142 [==============================] - 4s 3ms/sample - loss: 72732.1883 - val_loss: 197463.3644\n",
      "Epoch 4/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 60452.9719 - val_loss: 134761.0330\n",
      "Epoch 5/10\n",
      "1142/1142 [==============================] - 4s 3ms/sample - loss: 57096.3839 - val_loss: 151085.2057\n",
      "Epoch 6/10\n",
      "1142/1142 [==============================] - 3s 3ms/sample - loss: 51879.2138 - val_loss: 196586.3453\n",
      "Epoch 7/10\n",
      "1142/1142 [==============================] - 4s 3ms/sample - loss: 50756.2329 - val_loss: 140900.0314\n",
      "Epoch 8/10\n",
      "1142/1142 [==============================] - 4s 3ms/sample - loss: 47175.8745 - val_loss: 147402.2783\n",
      "Epoch 9/10\n",
      "1142/1142 [==============================] - 4s 3ms/sample - loss: 45838.7409 - val_loss: 169236.7944\n",
      "Epoch 10/10\n",
      "1142/1142 [==============================] - 4s 3ms/sample - loss: 43174.6528 - val_loss: 145238.9128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|███▉                                                                              | 1/21 [02:02<40:47, 122.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1141, 4, 18) (286, 4, 18) (1141,) (286,)\n",
      "Train on 1141 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1141/1141 [==============================] - 6s 5ms/sample - loss: 139325.2483 - val_loss: 32334.5344\n",
      "Epoch 2/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 28635.2304 - val_loss: 29966.9427\n",
      "Epoch 3/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 23259.9298 - val_loss: 26156.4149\n",
      "Epoch 4/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 23419.3230 - val_loss: 21347.2589\n",
      "Epoch 5/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 20940.2335 - val_loss: 21378.4368\n",
      "Epoch 6/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 21286.5281 - val_loss: 20491.7457\n",
      "Epoch 7/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 20119.4140 - val_loss: 24882.9697\n",
      "Epoch 8/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 20678.5146 - val_loss: 21589.2174\n",
      "Epoch 9/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 19709.8649 - val_loss: 23002.2824\n",
      "Epoch 10/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 18559.5550 - val_loss: 22044.0262\n",
      "(1141, 4, 18) (286, 4, 18) (1141,) (286,)\n",
      "Train on 1141 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1141/1141 [==============================] - 6s 6ms/sample - loss: 125120.1296 - val_loss: 45843.6406\n",
      "Epoch 2/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 33228.0222 - val_loss: 31326.0497\n",
      "Epoch 3/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 27805.0670 - val_loss: 29288.6861\n",
      "Epoch 4/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 26475.4833 - val_loss: 27851.0986\n",
      "Epoch 5/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 27738.5273 - val_loss: 28047.5964\n",
      "Epoch 6/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 25999.2948 - val_loss: 26707.8944\n",
      "Epoch 7/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 24839.7731 - val_loss: 27604.7850\n",
      "Epoch 8/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 24052.5761 - val_loss: 30019.3525\n",
      "Epoch 9/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 25268.6456 - val_loss: 26940.9913\n",
      "Epoch 10/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 23682.9726 - val_loss: 29202.0690\n",
      "(1141, 4, 18) (286, 4, 18) (1141,) (286,)\n",
      "Train on 1141 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1141/1141 [==============================] - 6s 5ms/sample - loss: 152415.6853 - val_loss: 55851.0139\n",
      "Epoch 2/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 46914.5066 - val_loss: 47908.7881\n",
      "Epoch 3/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 41746.7894 - val_loss: 39357.9655\n",
      "Epoch 4/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 37712.4785 - val_loss: 33529.9614\n",
      "Epoch 5/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 38436.4631 - val_loss: 38751.8330\n",
      "Epoch 6/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 35961.5163 - val_loss: 37877.1181\n",
      "Epoch 7/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 34136.3255 - val_loss: 34866.9505\n",
      "Epoch 8/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 36117.7768 - val_loss: 40052.2996\n",
      "Epoch 9/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 33224.4853 - val_loss: 35438.6425\n",
      "Epoch 10/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 33432.8999 - val_loss: 50648.6379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████▊                                                                          | 2/21 [03:52<36:30, 115.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1140, 4, 18) (286, 4, 18) (1140,) (286,)\n",
      "Train on 1140 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1140/1140 [==============================] - 7s 6ms/sample - loss: 217713.3683 - val_loss: 50908.5487\n",
      "Epoch 2/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 24337.2562 - val_loss: 12985.4784\n",
      "Epoch 3/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 14890.5481 - val_loss: 14588.9726- l\n",
      "Epoch 4/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 14193.4887 - val_loss: 17154.5892\n",
      "Epoch 5/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 12293.4429 - val_loss: 13244.9355\n",
      "Epoch 6/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 13277.6912 - val_loss: 17938.7788\n",
      "Epoch 7/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 11023.6889 - val_loss: 20515.1339\n",
      "Epoch 8/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 10850.1748 - val_loss: 14886.5803\n",
      "Epoch 9/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 10189.4760 - val_loss: 17032.7042\n",
      "Epoch 10/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 10583.4507 - val_loss: 15752.9390\n",
      "(1140, 4, 18) (286, 4, 18) (1140,) (286,)\n",
      "Train on 1140 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1140/1140 [==============================] - 8s 7ms/sample - loss: 209836.1727 - val_loss: 53298.7649\n",
      "Epoch 2/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 31817.6432 - val_loss: 65209.2917\n",
      "Epoch 3/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 24186.3381 - val_loss: 16796.1253\n",
      "Epoch 4/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 19191.3917 - val_loss: 16281.6793\n",
      "Epoch 5/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 19040.7692 - val_loss: 19780.1844\n",
      "Epoch 6/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 17606.8372 - val_loss: 23671.8690\n",
      "Epoch 7/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 17892.1538 - val_loss: 23332.9164\n",
      "Epoch 8/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 17683.8565 - val_loss: 19049.0387\n",
      "Epoch 9/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 20158.3712 - val_loss: 24050.3958\n",
      "Epoch 10/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 16923.9872 - val_loss: 17253.6353\n",
      "(1140, 4, 18) (286, 4, 18) (1140,) (286,)\n",
      "Train on 1140 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1140/1140 [==============================] - 7s 7ms/sample - loss: 186719.6306 - val_loss: 48784.5483\n",
      "Epoch 2/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 36105.4504 - val_loss: 33523.0475\n",
      "Epoch 3/10\n",
      "1140/1140 [==============================] - 4s 3ms/sample - loss: 30555.3885 - val_loss: 27970.1811\n",
      "Epoch 4/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 26126.9894 - val_loss: 25516.6593\n",
      "Epoch 5/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 27049.4021 - val_loss: 33046.4230\n",
      "Epoch 6/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 25192.4461 - val_loss: 33114.3503\n",
      "Epoch 7/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 23989.7302 - val_loss: 26234.2841\n",
      "Epoch 8/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 24604.2041 - val_loss: 33490.2913\n",
      "Epoch 9/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 21079.5374 - val_loss: 33043.3468\n",
      "Epoch 10/10\n",
      "1140/1140 [==============================] - 3s 3ms/sample - loss: 21902.5456 - val_loss: 34865.9002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|███████████▋                                                                      | 3/21 [05:52<35:09, 117.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1125, 4, 18) (282, 4, 18) (1125,) (282,)\n",
      "Train on 1125 samples, validate on 282 samples\n",
      "Epoch 1/10\n",
      "1125/1125 [==============================] - 7s 6ms/sample - loss: 87586903.0378 - val_loss: 447741037.6897\n",
      "Epoch 2/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 44162653.5639 - val_loss: 202507441.4628\n",
      "Epoch 3/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 37326419.4194 - val_loss: 185225957.6197\n",
      "Epoch 4/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 35434844.0354 - val_loss: 183866631.5780\n",
      "Epoch 5/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 34351886.2683 - val_loss: 179179943.4402\n",
      "Epoch 6/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 33811625.0180 - val_loss: 177220196.6157\n",
      "Epoch 7/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 33302896.9016 - val_loss: 175863638.7704\n",
      "Epoch 8/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 33382346.8645 - val_loss: 178469748.7509\n",
      "Epoch 9/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 32983979.7162 - val_loss: 177712057.9570\n",
      "Epoch 10/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 33507576.9803 - val_loss: 174407236.1294\n",
      "(1125, 4, 18) (282, 4, 18) (1125,) (282,)\n",
      "Train on 1125 samples, validate on 282 samples\n",
      "Epoch 1/10\n",
      "1125/1125 [==============================] - 7s 6ms/sample - loss: 99727684.4916 - val_loss: 278153611.7447\n",
      "Epoch 2/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 45146888.6352 - val_loss: 240203419.9211\n",
      "Epoch 3/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 37603375.4088 - val_loss: 194793564.1782\n",
      "Epoch 4/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 36006866.8281 - val_loss: 190882884.7278\n",
      "Epoch 5/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 35122616.9237 - val_loss: 182991153.3821\n",
      "Epoch 6/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 34493011.1537 - val_loss: 186521129.8972\n",
      "Epoch 7/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 33629142.9005 - val_loss: 179345508.0991\n",
      "Epoch 8/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 33235840.3055 - val_loss: 184917783.3050\n",
      "Epoch 9/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 33161276.7904 - val_loss: 180627219.7519\n",
      "Epoch 10/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 33577768.3207 - val_loss: 189906170.6463\n",
      "(1125, 4, 18) (282, 4, 18) (1125,) (282,)\n",
      "Train on 1125 samples, validate on 282 samples\n",
      "Epoch 1/10\n",
      "1125/1125 [==============================] - 8s 7ms/sample - loss: 82563335.2809 - val_loss: 269216698.9131\n",
      "Epoch 2/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 43115058.1202 - val_loss: 210076829.1082\n",
      "Epoch 3/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 38075625.7923 - val_loss: 195019861.1746\n",
      "Epoch 4/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 37291764.0451 - val_loss: 188520594.0253\n",
      "Epoch 5/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 35649024.0343 - val_loss: 185633856.2770\n",
      "Epoch 6/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 35376191.5367 - val_loss: 181015310.8573\n",
      "Epoch 7/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 35221468.2647 - val_loss: 193857632.2154\n",
      "Epoch 8/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 34732806.4767 - val_loss: 189705715.5399\n",
      "Epoch 9/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 34338539.7572 - val_loss: 179934115.5098\n",
      "Epoch 10/10\n",
      "1125/1125 [==============================] - 3s 3ms/sample - loss: 34358797.9740 - val_loss: 197740435.4459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████████████▌                                                                  | 4/21 [07:47<32:57, 116.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1132, 4, 18) (284, 4, 18) (1132,) (284,)\n",
      "Train on 1132 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1132/1132 [==============================] - 7s 6ms/sample - loss: 7433714.0206 - val_loss: 1234321.1773\n",
      "Epoch 2/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 458508.7852 - val_loss: 215114.4900s: 461819.14\n",
      "Epoch 3/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 236814.3949 - val_loss: 199056.6227\n",
      "Epoch 4/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 185339.6115 - val_loss: 180145.5942\n",
      "Epoch 5/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 178912.5607 - val_loss: 197553.6608\n",
      "Epoch 6/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 183238.4515 - val_loss: 171685.2599\n",
      "Epoch 7/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 173078.7952 - val_loss: 176094.5055\n",
      "Epoch 8/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 170451.0744 - val_loss: 240669.8257\n",
      "Epoch 9/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 169064.8458 - val_loss: 174377.6242\n",
      "Epoch 10/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 175050.3313 - val_loss: 176915.2959\n",
      "(1132, 4, 18) (284, 4, 18) (1132,) (284,)\n",
      "Train on 1132 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1132/1132 [==============================] - 7s 6ms/sample - loss: 7863195.8712 - val_loss: 615990.8031\n",
      "Epoch 2/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 559597.6589 - val_loss: 432666.1167\n",
      "Epoch 3/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 282188.3329 - val_loss: 312571.0052\n",
      "Epoch 4/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 238166.7517 - val_loss: 293519.4576\n",
      "Epoch 5/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 233173.9276 - val_loss: 255317.8491\n",
      "Epoch 6/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 217952.1180 - val_loss: 253048.2432\n",
      "Epoch 7/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 216790.8949 - val_loss: 310410.7795\n",
      "Epoch 8/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 209819.2289 - val_loss: 255718.9058\n",
      "Epoch 9/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 214922.4885 - val_loss: 293241.7232\n",
      "Epoch 10/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 212985.1514 - val_loss: 280192.9862\n",
      "(1132, 4, 18) (284, 4, 18) (1132,) (284,)\n",
      "Train on 1132 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1132/1132 [==============================] - 7s 7ms/sample - loss: 7209295.0359 - val_loss: 1188819.8910\n",
      "Epoch 2/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 567252.8749 - val_loss: 536649.1000\n",
      "Epoch 3/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 402631.8950 - val_loss: 463454.5847\n",
      "Epoch 4/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 349804.7824 - val_loss: 460404.5704\n",
      "Epoch 5/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 326220.7025 - val_loss: 526911.4694\n",
      "Epoch 6/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 309123.8978 - val_loss: 524269.0333\n",
      "Epoch 7/10\n",
      "1132/1132 [==============================] - 4s 3ms/sample - loss: 314843.4084 - val_loss: 561640.5155\n",
      "Epoch 8/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 300822.3005 - val_loss: 510122.1436\n",
      "Epoch 9/10\n",
      "1132/1132 [==============================] - 4s 3ms/sample - loss: 295071.9372 - val_loss: 573260.3241\n",
      "Epoch 10/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 282303.9332 - val_loss: 574091.3872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████▌                                                              | 5/21 [09:44<31:08, 116.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1141, 4, 18) (286, 4, 18) (1141,) (286,)\n",
      "Train on 1141 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1141/1141 [==============================] - 7s 6ms/sample - loss: 664500.0687 - val_loss: 160279.6489\n",
      "Epoch 2/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 106475.3467 - val_loss: 95644.0720\n",
      "Epoch 3/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 82514.5262 - val_loss: 83175.8136\n",
      "Epoch 4/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 74191.1184 - val_loss: 76862.4929\n",
      "Epoch 5/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 76060.8982 - val_loss: 86622.7357\n",
      "Epoch 6/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 70990.7150 - val_loss: 72828.3121\n",
      "Epoch 7/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 65876.6687 - val_loss: 79003.4243\n",
      "Epoch 8/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 68170.8693 - val_loss: 81036.6857\n",
      "Epoch 9/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 64973.1129 - val_loss: 74371.4729\n",
      "Epoch 10/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 63271.3541 - val_loss: 88821.3134\n",
      "(1141, 4, 18) (286, 4, 18) (1141,) (286,)\n",
      "Train on 1141 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1141/1141 [==============================] - 6s 5ms/sample - loss: 762067.2426 - val_loss: 233374.8763\n",
      "Epoch 2/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 132473.9014 - val_loss: 118468.6232\n",
      "Epoch 3/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 101219.6799 - val_loss: 120263.0647\n",
      "Epoch 4/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 95225.0791 - val_loss: 111703.2092\n",
      "Epoch 5/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 86976.0592 - val_loss: 90288.1331\n",
      "Epoch 6/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 83332.2578 - val_loss: 91032.5354\n",
      "Epoch 7/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 81172.6196 - val_loss: 86656.3058\n",
      "Epoch 8/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 83398.4920 - val_loss: 101173.4858\n",
      "Epoch 9/10\n",
      "1141/1141 [==============================] - 3s 2ms/sample - loss: 79432.3659 - val_loss: 106173.2124\n",
      "Epoch 10/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 75645.7236 - val_loss: 95555.0499\n",
      "(1141, 4, 18) (286, 4, 18) (1141,) (286,)\n",
      "Train on 1141 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1141/1141 [==============================] - 6s 5ms/sample - loss: 677991.5310 - val_loss: 220314.8732\n",
      "Epoch 2/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 157020.2392 - val_loss: 153329.2528\n",
      "Epoch 3/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 131819.1832 - val_loss: 158718.2239\n",
      "Epoch 4/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 122846.3654 - val_loss: 127922.2977\n",
      "Epoch 5/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 115213.5678 - val_loss: 136726.4499\n",
      "Epoch 6/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 108682.5869 - val_loss: 121577.4088\n",
      "Epoch 7/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 112815.8971 - val_loss: 159127.9161\n",
      "Epoch 8/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 108865.8997 - val_loss: 106401.8649\n",
      "Epoch 9/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 106244.2730 - val_loss: 157488.2023\n",
      "Epoch 10/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 102934.7810 - val_loss: 116519.0811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|███████████████████████▍                                                          | 6/21 [11:31<28:19, 113.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1136, 4, 18) (284, 4, 18) (1136,) (284,)\n",
      "Train on 1136 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1136/1136 [==============================] - 7s 6ms/sample - loss: 424085.1723 - val_loss: 126185.3317\n",
      "Epoch 2/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 108421.8776 - val_loss: 120075.2407\n",
      "Epoch 3/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 88688.9121 - val_loss: 87890.9113\n",
      "Epoch 4/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 81031.5874 - val_loss: 69551.4637\n",
      "Epoch 5/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 75660.8305 - val_loss: 77048.8161\n",
      "Epoch 6/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 78717.4289 - val_loss: 72598.6223\n",
      "Epoch 7/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 78766.1815 - val_loss: 70626.1203\n",
      "Epoch 8/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 72253.3588 - val_loss: 79806.9912\n",
      "Epoch 9/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 71527.3080 - val_loss: 70329.1727\n",
      "Epoch 10/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 72254.7946 - val_loss: 75182.9989\n",
      "(1136, 4, 18) (284, 4, 18) (1136,) (284,)\n",
      "Train on 1136 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1136/1136 [==============================] - 7s 6ms/sample - loss: 404226.5537 - val_loss: 204944.6366\n",
      "Epoch 2/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 109085.6581 - val_loss: 134666.3396\n",
      "Epoch 3/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 96471.9111 - val_loss: 157779.8010\n",
      "Epoch 4/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 88061.7010 - val_loss: 143689.3832\n",
      "Epoch 5/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 90870.7389 - val_loss: 166535.0036\n",
      "Epoch 6/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 86598.0030 - val_loss: 121756.5812\n",
      "Epoch 7/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 85343.8858 - val_loss: 119314.6341\n",
      "Epoch 8/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 85089.2920 - val_loss: 116615.0196\n",
      "Epoch 9/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 81006.3053 - val_loss: 148108.8449\n",
      "Epoch 10/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 78433.3782 - val_loss: 136184.4344\n",
      "(1136, 4, 18) (284, 4, 18) (1136,) (284,)\n",
      "Train on 1136 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1136/1136 [==============================] - 7s 6ms/sample - loss: 470166.5859 - val_loss: 280134.5220\n",
      "Epoch 2/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 146618.0446 - val_loss: 195789.5341\n",
      "Epoch 3/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 123386.5751 - val_loss: 152120.1189\n",
      "Epoch 4/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 120282.4335 - val_loss: 217375.2979\n",
      "Epoch 5/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 111924.9264 - val_loss: 130127.2447\n",
      "Epoch 6/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 115625.8335 - val_loss: 205349.5202\n",
      "Epoch 7/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 109765.7860 - val_loss: 235426.5993\n",
      "Epoch 8/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 103999.1060 - val_loss: 207893.7590\n",
      "Epoch 9/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 104325.5619 - val_loss: 249360.2571\n",
      "Epoch 10/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 100169.2722 - val_loss: 158208.2734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████████████████▎                                                      | 7/21 [13:27<26:41, 114.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1138, 4, 18) (285, 4, 18) (1138,) (285,)\n",
      "Train on 1138 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1138/1138 [==============================] - 8s 7ms/sample - loss: 145368.6218 - val_loss: 61041.3824\n",
      "Epoch 2/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 44359.4681 - val_loss: 32171.8885\n",
      "Epoch 3/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 30639.8731 - val_loss: 32380.8775\n",
      "Epoch 4/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 28155.1418 - val_loss: 24192.5386\n",
      "Epoch 5/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 25191.7626 - val_loss: 25983.4892\n",
      "Epoch 6/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 25026.9091 - val_loss: 25455.1322\n",
      "Epoch 7/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 23892.4397 - val_loss: 27147.2468\n",
      "Epoch 8/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 22993.3532 - val_loss: 27477.6183\n",
      "Epoch 9/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 22834.1569 - val_loss: 33425.7092\n",
      "Epoch 10/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 20877.4058 - val_loss: 27235.7992\n",
      "(1138, 4, 18) (285, 4, 18) (1138,) (285,)\n",
      "Train on 1138 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1138/1138 [==============================] - 7s 6ms/sample - loss: 162517.6067 - val_loss: 99305.5024\n",
      "Epoch 2/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 54597.0846 - val_loss: 76200.3272\n",
      "Epoch 3/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 43372.4191 - val_loss: 61834.4096\n",
      "Epoch 4/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 42145.1502 - val_loss: 45449.6190\n",
      "Epoch 5/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 37437.3340 - val_loss: 36643.6999\n",
      "Epoch 6/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 38108.3462 - val_loss: 39027.6737\n",
      "Epoch 7/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 37682.4982 - val_loss: 49067.3363\n",
      "Epoch 8/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 35194.5977 - val_loss: 39886.4027\n",
      "Epoch 9/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 33432.0000 - val_loss: 38604.1273\n",
      "Epoch 10/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 33102.1998 - val_loss: 39998.1795\n",
      "(1138, 4, 18) (285, 4, 18) (1138,) (285,)\n",
      "Train on 1138 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1138/1138 [==============================] - 7s 6ms/sample - loss: 170066.0306 - val_loss: 203191.4766\n",
      "Epoch 2/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 70911.4556 - val_loss: 102309.8814\n",
      "Epoch 3/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 59551.2753 - val_loss: 106941.1275\n",
      "Epoch 4/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 55183.8672 - val_loss: 89718.5446\n",
      "Epoch 5/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 52675.0047 - val_loss: 84625.9172\n",
      "Epoch 6/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 48538.5679 - val_loss: 112688.4572\n",
      "Epoch 7/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 46527.9640 - val_loss: 124028.2042\n",
      "Epoch 8/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 46110.1291 - val_loss: 109621.1476\n",
      "Epoch 9/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 41634.2398 - val_loss: 99148.6157\n",
      "Epoch 10/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 41200.6039 - val_loss: 74246.6462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▏                                                  | 8/21 [15:24<24:54, 114.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1141, 4, 18) (286, 4, 18) (1141,) (286,)\n",
      "Train on 1141 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1141/1141 [==============================] - 6s 5ms/sample - loss: 15479922.1135 - val_loss: 9310597.2754\n",
      "Epoch 2/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 3009614.6400 - val_loss: 6961062.5004\n",
      "Epoch 3/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 2217270.3207 - val_loss: 7741472.4240\n",
      "Epoch 4/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 2043276.0706 - val_loss: 6209531.8349\n",
      "Epoch 5/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 1972799.9232 - val_loss: 7101881.8123\n",
      "Epoch 6/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 1955570.6494 - val_loss: 6246472.0065\n",
      "Epoch 7/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 1977870.5307 - val_loss: 6895079.6947\n",
      "Epoch 8/10\n",
      "1141/1141 [==============================] - 3s 2ms/sample - loss: 1867492.2805 - val_loss: 7539113.9843\n",
      "Epoch 9/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 1840573.3056 - val_loss: 6232274.2262\n",
      "Epoch 10/10\n",
      "1141/1141 [==============================] - 3s 2ms/sample - loss: 1777341.0624 - val_loss: 6194961.8150\n",
      "(1141, 4, 18) (286, 4, 18) (1141,) (286,)\n",
      "Train on 1141 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1141/1141 [==============================] - 6s 5ms/sample - loss: 12884597.9001 - val_loss: 9265146.1475\n",
      "Epoch 2/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 2707730.8288 - val_loss: 7061109.9638\n",
      "Epoch 3/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 2361629.1840 - val_loss: 7760157.4691\n",
      "Epoch 4/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 2165089.4786 - val_loss: 6653412.2329\n",
      "Epoch 5/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 2071322.4077 - val_loss: 7731972.3018\n",
      "Epoch 6/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 2100343.2497 - val_loss: 7890818.8145\n",
      "Epoch 7/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 2059040.2887 - val_loss: 7325122.4381\n",
      "Epoch 8/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 2029649.0234 - val_loss: 7231694.0656\n",
      "Epoch 9/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 1952541.8415 - val_loss: 7591921.9761\n",
      "Epoch 10/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 1908367.6964 - val_loss: 8520411.5748\n",
      "(1141, 4, 18) (286, 4, 18) (1141,) (286,)\n",
      "Train on 1141 samples, validate on 286 samples\n",
      "Epoch 1/10\n",
      "1141/1141 [==============================] - 6s 5ms/sample - loss: 13371619.8566 - val_loss: 14235060.2313\n",
      "Epoch 2/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 3094524.3067 - val_loss: 10209998.4139\n",
      "Epoch 3/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 2642146.1831 - val_loss: 9739986.5988\n",
      "Epoch 4/10\n",
      "1141/1141 [==============================] - 3s 2ms/sample - loss: 2544768.8367 - val_loss: 9595373.9883\n",
      "Epoch 5/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 2433416.5445 - val_loss: 10419722.5100\n",
      "Epoch 6/10\n",
      "1141/1141 [==============================] - 3s 2ms/sample - loss: 2398867.2367 - val_loss: 9756726.9370\n",
      "Epoch 7/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 2333284.0735 - val_loss: 8401849.0162\n",
      "Epoch 8/10\n",
      "1141/1141 [==============================] - 3s 2ms/sample - loss: 2292129.5117 - val_loss: 10034644.8725\n",
      "Epoch 9/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 2266339.0553 - val_loss: 10953889.4775\n",
      "Epoch 10/10\n",
      "1141/1141 [==============================] - 3s 3ms/sample - loss: 2203462.2487 - val_loss: 10263708.5675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|███████████████████████████████████▏                                              | 9/21 [17:07<22:15, 111.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1139, 4, 18) (285, 4, 18) (1139,) (285,)\n",
      "Train on 1139 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1139/1139 [==============================] - 8s 7ms/sample - loss: 4653788.0534 - val_loss: 705847.7972\n",
      "Epoch 2/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 1362458.3371 - val_loss: 1726241.1681\n",
      "Epoch 3/10\n",
      "1139/1139 [==============================] - 4s 3ms/sample - loss: 1203461.1939 - val_loss: 526454.0653\n",
      "Epoch 4/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 873355.4258 - val_loss: 872956.1769\n",
      "Epoch 5/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 760882.8997 - val_loss: 396161.0559\n",
      "Epoch 6/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 744180.2399 - val_loss: 500102.7828\n",
      "Epoch 7/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 722022.0644 - val_loss: 600046.2607\n",
      "Epoch 8/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 753477.7037 - val_loss: 527683.8873\n",
      "Epoch 9/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 736951.9427 - val_loss: 760502.3800\n",
      "Epoch 10/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 743314.4739 - val_loss: 468261.5040\n",
      "(1139, 4, 18) (285, 4, 18) (1139,) (285,)\n",
      "Train on 1139 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1139/1139 [==============================] - 7s 6ms/sample - loss: 4512737.3507 - val_loss: 856869.5298\n",
      "Epoch 2/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 1279212.4851 - val_loss: 1413145.7298\n",
      "Epoch 3/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 1187738.4436 - val_loss: 1092126.2554\n",
      "Epoch 4/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 966137.0369 - val_loss: 1151019.2243\n",
      "Epoch 5/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 867229.1392 - val_loss: 1294676.1872\n",
      "Epoch 6/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 896853.2175 - val_loss: 1094720.1332\n",
      "Epoch 7/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 900510.1367 - val_loss: 2592165.4834\n",
      "Epoch 8/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 876532.7292 - val_loss: 1295611.4466\n",
      "Epoch 9/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 896286.0578 - val_loss: 1693641.6888\n",
      "Epoch 10/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 786672.1892 - val_loss: 1537533.2716\n",
      "(1139, 4, 18) (285, 4, 18) (1139,) (285,)\n",
      "Train on 1139 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1139/1139 [==============================] - 7s 6ms/sample - loss: 4721721.5395 - val_loss: 1662185.9530\n",
      "Epoch 2/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 1496434.4419 - val_loss: 1796250.9531\n",
      "Epoch 3/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 1338643.3587 - val_loss: 2170494.6144\n",
      "Epoch 4/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 1370147.2917 - val_loss: 1743188.8086\n",
      "Epoch 5/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 1186758.5205 - val_loss: 1877313.7736\n",
      "Epoch 6/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 1165263.4954 - val_loss: 2024915.1977\n",
      "Epoch 7/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 1119061.4124 - val_loss: 1830804.9145\n",
      "Epoch 8/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 1112893.2595 - val_loss: 1807681.6828\n",
      "Epoch 9/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 1149789.0021 - val_loss: 1950913.3547\n",
      "Epoch 10/10\n",
      "1139/1139 [==============================] - 3s 3ms/sample - loss: 1119825.9658 - val_loss: 1783158.2998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|██████████████████████████████████████▌                                          | 10/21 [19:07<20:53, 113.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1136, 4, 18) (285, 4, 18) (1136,) (285,)\n",
      "Train on 1136 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1136/1136 [==============================] - 8s 7ms/sample - loss: 3240980.4917 - val_loss: 1361792.9170\n",
      "Epoch 2/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 1013521.3898 - val_loss: 863088.7570\n",
      "Epoch 3/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 868151.6579 - val_loss: 988534.1698\n",
      "Epoch 4/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 835127.7900 - val_loss: 966696.2338\n",
      "Epoch 5/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 749192.6947 - val_loss: 819768.8165\n",
      "Epoch 6/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 723298.2500 - val_loss: 1000491.1378\n",
      "Epoch 7/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 750183.3164 - val_loss: 796869.5234\n",
      "Epoch 8/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 730178.4961 - val_loss: 938527.8167\n",
      "Epoch 9/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 683918.1367 - val_loss: 1115363.1938\n",
      "Epoch 10/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 721206.2216 - val_loss: 838046.3863\n",
      "(1136, 4, 18) (285, 4, 18) (1136,) (285,)\n",
      "Train on 1136 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1136/1136 [==============================] - 7s 6ms/sample - loss: 3177372.3388 - val_loss: 1228492.9490\n",
      "Epoch 2/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 987177.8056 - val_loss: 941889.1798\n",
      "Epoch 3/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 880799.4213 - val_loss: 1140894.1279\n",
      "Epoch 4/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 879111.9114 - val_loss: 889485.3459\n",
      "Epoch 5/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 845465.1968 - val_loss: 1241152.2570\n",
      "Epoch 6/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 819052.2987 - val_loss: 829451.8982\n",
      "Epoch 7/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 833664.9343 - val_loss: 1025357.0032\n",
      "Epoch 8/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 797024.6788 - val_loss: 910025.2840\n",
      "Epoch 9/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 776490.1940 - val_loss: 1031977.6064\n",
      "Epoch 10/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 803964.5536 - val_loss: 844788.3488\n",
      "(1136, 4, 18) (285, 4, 18) (1136,) (285,)\n",
      "Train on 1136 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1136/1136 [==============================] - 7s 6ms/sample - loss: 3976198.2128 - val_loss: 2488267.4946\n",
      "Epoch 2/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 1224588.7349 - val_loss: 1592226.5412\n",
      "Epoch 3/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 949225.6201 - val_loss: 1312654.4895\n",
      "Epoch 4/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 833539.4059 - val_loss: 1519842.9680\n",
      "Epoch 5/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 806876.1778 - val_loss: 1236667.6406\n",
      "Epoch 6/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 725250.2994 - val_loss: 1592967.7145\n",
      "Epoch 7/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 769361.6300 - val_loss: 1052866.8161\n",
      "Epoch 8/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 728757.7178 - val_loss: 1215108.8695\n",
      "Epoch 9/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 709197.4104 - val_loss: 1045701.1645\n",
      "Epoch 10/10\n",
      "1136/1136 [==============================] - 3s 3ms/sample - loss: 694475.5194 - val_loss: 1246416.7685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|██████████████████████████████████████████▍                                      | 11/21 [21:02<19:02, 114.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1135, 4, 18) (284, 4, 18) (1135,) (284,)\n",
      "Train on 1135 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1135/1135 [==============================] - 7s 6ms/sample - loss: 349369.9566 - val_loss: 64367.0030\n",
      "Epoch 2/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 40624.6346 - val_loss: 38441.5149\n",
      "Epoch 3/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 28919.9334 - val_loss: 52737.3139\n",
      "Epoch 4/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 25785.9439 - val_loss: 32198.5636\n",
      "Epoch 5/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 26961.7875 - val_loss: 29349.5846\n",
      "Epoch 6/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 26603.9783 - val_loss: 44276.5344\n",
      "Epoch 7/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 25266.0743 - val_loss: 25626.8303\n",
      "Epoch 8/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 22839.9524 - val_loss: 26269.0095\n",
      "Epoch 9/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 22700.8107 - val_loss: 34543.7843\n",
      "Epoch 10/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 23192.5870 - val_loss: 29423.5585\n",
      "(1135, 4, 18) (284, 4, 18) (1135,) (284,)\n",
      "Train on 1135 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1135/1135 [==============================] - 7s 6ms/sample - loss: 359486.5472 - val_loss: 137619.0888\n",
      "Epoch 2/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 55015.4616 - val_loss: 70528.2748\n",
      "Epoch 3/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 37418.3165 - val_loss: 70155.8828\n",
      "Epoch 4/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 36635.1029 - val_loss: 42793.4448\n",
      "Epoch 5/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 32230.4921 - val_loss: 48319.1680\n",
      "Epoch 6/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 32479.9654 - val_loss: 65805.9566\n",
      "Epoch 7/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 32819.4601 - val_loss: 56417.7949\n",
      "Epoch 8/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 29239.4961 - val_loss: 41289.7226\n",
      "Epoch 9/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 30831.2883 - val_loss: 41461.7096\n",
      "Epoch 10/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 26946.5735 - val_loss: 46619.3366\n",
      "(1135, 4, 18) (284, 4, 18) (1135,) (284,)\n",
      "Train on 1135 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1135/1135 [==============================] - 8s 7ms/sample - loss: 439983.6275 - val_loss: 235075.0401\n",
      "Epoch 2/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 74647.3266 - val_loss: 106606.3170\n",
      "Epoch 3/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 53513.3996 - val_loss: 78438.5727\n",
      "Epoch 4/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 47979.1736 - val_loss: 86419.3310\n",
      "Epoch 5/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 45640.0509 - val_loss: 78078.0606\n",
      "Epoch 6/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 41203.3997 - val_loss: 81222.6482\n",
      "Epoch 7/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 40592.5986 - val_loss: 82241.0582\n",
      "Epoch 8/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 40060.4537 - val_loss: 74800.8792\n",
      "Epoch 9/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 39942.1037 - val_loss: 59477.5539\n",
      "Epoch 10/10\n",
      "1135/1135 [==============================] - 3s 3ms/sample - loss: 38293.9913 - val_loss: 83257.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|██████████████████████████████████████████████▎                                  | 12/21 [22:57<17:10, 114.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1132, 4, 18) (284, 4, 18) (1132,) (284,)\n",
      "Train on 1132 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1132/1132 [==============================] - 7s 6ms/sample - loss: 6331417.8313 - val_loss: 2638836.5659\n",
      "Epoch 2/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1513484.8668 - val_loss: 1096351.1918\n",
      "Epoch 3/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1063049.4216 - val_loss: 1091410.4467\n",
      "Epoch 4/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1025047.4552 - val_loss: 793303.5823\n",
      "Epoch 5/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 985109.0428 - val_loss: 1002638.5907\n",
      "Epoch 6/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 941295.1965 - val_loss: 732516.3865\n",
      "Epoch 7/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 909078.8694 - val_loss: 758235.4123\n",
      "Epoch 8/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 918787.8560 - val_loss: 862700.2906\n",
      "Epoch 9/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 879814.6730 - val_loss: 774089.5711\n",
      "Epoch 10/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 833564.1527 - val_loss: 801315.7538\n",
      "(1132, 4, 18) (284, 4, 18) (1132,) (284,)\n",
      "Train on 1132 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1132/1132 [==============================] - 7s 6ms/sample - loss: 5756410.0368 - val_loss: 2095971.1798\n",
      "Epoch 2/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1607837.2227 - val_loss: 1142851.4258\n",
      "Epoch 3/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1335763.1661 - val_loss: 1152777.0912\n",
      "Epoch 4/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1297416.8723 - val_loss: 1464077.4881\n",
      "Epoch 5/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1294244.8133 - val_loss: 1138935.1942\n",
      "Epoch 6/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1245341.9200 - val_loss: 1213153.4636\n",
      "Epoch 7/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1227993.7824 - val_loss: 1371196.5423\n",
      "Epoch 8/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1222675.8323 - val_loss: 1071338.5274\n",
      "Epoch 9/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1162255.7628 - val_loss: 2019316.0184\n",
      "Epoch 10/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1156654.9319 - val_loss: 1385772.0155\n",
      "(1132, 4, 18) (284, 4, 18) (1132,) (284,)\n",
      "Train on 1132 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1132/1132 [==============================] - 7s 6ms/sample - loss: 5767466.0752 - val_loss: 5395905.1352\n",
      "Epoch 2/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1688943.0953 - val_loss: 3553830.7173\n",
      "Epoch 3/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1411274.8098 - val_loss: 2765295.2007\n",
      "Epoch 4/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1398127.5985 - val_loss: 2897046.2935\n",
      "Epoch 5/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1321644.0713 - val_loss: 2877957.3970\n",
      "Epoch 6/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1306667.8940 - val_loss: 2761182.1184\n",
      "Epoch 7/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1354209.2396 - val_loss: 3102852.0003\n",
      "Epoch 8/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1245293.8287 - val_loss: 2691770.6486\n",
      "Epoch 9/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1292846.1587 - val_loss: 2664011.2903\n",
      "Epoch 10/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 1215652.8530 - val_loss: 3302932.5060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████████████████████████████████████████████████▏                              | 13/21 [24:49<15:11, 113.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1138, 4, 18) (285, 4, 18) (1138,) (285,)\n",
      "Train on 1138 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1138/1138 [==============================] - 7s 6ms/sample - loss: 1807137.7294 - val_loss: 282529.8673\n",
      "Epoch 2/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 155144.5508 - val_loss: 108970.8776\n",
      "Epoch 3/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 85303.5578 - val_loss: 77100.3074\n",
      "Epoch 4/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 79785.6035 - val_loss: 86921.9598\n",
      "Epoch 5/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 71252.0551 - val_loss: 76388.6653\n",
      "Epoch 6/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 66184.4694 - val_loss: 77500.1751\n",
      "Epoch 7/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 68069.3509 - val_loss: 81219.9229\n",
      "Epoch 8/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 64484.9355 - val_loss: 81276.4717\n",
      "Epoch 9/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 62079.3041 - val_loss: 81473.1271\n",
      "Epoch 10/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 62253.9345 - val_loss: 78072.1796\n",
      "(1138, 4, 18) (285, 4, 18) (1138,) (285,)\n",
      "Train on 1138 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1138/1138 [==============================] - 7s 6ms/sample - loss: 1803904.5466 - val_loss: 305405.1448\n",
      "Epoch 2/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 156987.2560 - val_loss: 137098.2144\n",
      "Epoch 3/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 107653.6736 - val_loss: 110567.3875\n",
      "Epoch 4/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 99196.6519 - val_loss: 107407.5158\n",
      "Epoch 5/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 98539.2049 - val_loss: 102515.5276\n",
      "Epoch 6/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 94761.7551 - val_loss: 108271.5533\n",
      "Epoch 7/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 99414.5066 - val_loss: 98714.6196\n",
      "Epoch 8/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 89174.0207 - val_loss: 97399.6850\n",
      "Epoch 9/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 94181.0419 - val_loss: 93568.5342\n",
      "Epoch 10/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 89741.8701 - val_loss: 102394.1147\n",
      "(1138, 4, 18) (285, 4, 18) (1138,) (285,)\n",
      "Train on 1138 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1138/1138 [==============================] - 7s 6ms/sample - loss: 1984873.3709 - val_loss: 474351.9352\n",
      "Epoch 2/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 180873.9859 - val_loss: 202069.0204\n",
      "Epoch 3/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 115740.7433 - val_loss: 144740.1752\n",
      "Epoch 4/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 107584.7473 - val_loss: 164652.9013\n",
      "Epoch 5/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 108547.9405 - val_loss: 141154.5275\n",
      "Epoch 6/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 107834.5873 - val_loss: 241744.2177\n",
      "Epoch 7/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 113542.7123 - val_loss: 163009.1439\n",
      "Epoch 8/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 102465.4277 - val_loss: 141374.7643\n",
      "Epoch 9/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 105135.1294 - val_loss: 155574.4297\n",
      "Epoch 10/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 106727.2918 - val_loss: 162570.5810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████████████████████████████                           | 14/21 [26:43<13:17, 113.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1132, 4, 18) (284, 4, 18) (1132,) (284,)\n",
      "Train on 1132 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1132/1132 [==============================] - 7s 7ms/sample - loss: 1138517.1621 - val_loss: 422657.6336\n",
      "Epoch 2/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 207168.3281 - val_loss: 278881.8281\n",
      "Epoch 3/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 148686.8232 - val_loss: 261354.6445\n",
      "Epoch 4/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 130260.4755 - val_loss: 254388.8614\n",
      "Epoch 5/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 126478.2538 - val_loss: 283862.3200\n",
      "Epoch 6/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 120194.6955 - val_loss: 253358.0628\n",
      "Epoch 7/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 117248.2095 - val_loss: 257640.5593\n",
      "Epoch 8/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 110379.6368 - val_loss: 285737.2398\n",
      "Epoch 9/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 119303.6800 - val_loss: 307386.1315\n",
      "Epoch 10/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 112049.2334 - val_loss: 280680.3784\n",
      "(1132, 4, 18) (284, 4, 18) (1132,) (284,)\n",
      "Train on 1132 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1132/1132 [==============================] - 7s 6ms/sample - loss: 1244178.4401 - val_loss: 451980.2531\n",
      "Epoch 2/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 232178.9446 - val_loss: 384673.8948\n",
      "Epoch 3/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 180369.1212 - val_loss: 350685.5538\n",
      "Epoch 4/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 163419.4720 - val_loss: 346003.3532\n",
      "Epoch 5/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 154781.8601 - val_loss: 425307.5604\n",
      "Epoch 6/10\n",
      "1132/1132 [==============================] - 4s 3ms/sample - loss: 155773.3050 - val_loss: 416004.7817\n",
      "Epoch 7/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 145451.9645 - val_loss: 400811.3846\n",
      "Epoch 8/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 140777.8738 - val_loss: 435756.5196\n",
      "Epoch 9/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 140878.9083 - val_loss: 373037.2662\n",
      "Epoch 10/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 135076.8052 - val_loss: 377170.2190\n",
      "(1132, 4, 18) (284, 4, 18) (1132,) (284,)\n",
      "Train on 1132 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1132/1132 [==============================] - 7s 7ms/sample - loss: 1196083.0384 - val_loss: 607698.9193\n",
      "Epoch 2/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 223633.6383 - val_loss: 571111.5193\n",
      "Epoch 3/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 183806.0373 - val_loss: 550947.0989\n",
      "Epoch 4/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 176597.1280 - val_loss: 510499.2295\n",
      "Epoch 5/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 176453.2567 - val_loss: 692928.5975\n",
      "Epoch 6/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 167657.4546 - val_loss: 473032.6656\n",
      "Epoch 7/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 162168.9183 - val_loss: 499915.0937\n",
      "Epoch 8/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 158019.1310 - val_loss: 547453.1240\n",
      "Epoch 9/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 160135.7416 - val_loss: 841093.3036\n",
      "Epoch 10/10\n",
      "1132/1132 [==============================] - 3s 3ms/sample - loss: 153445.9549 - val_loss: 692850.2971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|█████████████████████████████████████████████████████████▊                       | 15/21 [28:40<11:28, 114.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1138, 4, 18) (285, 4, 18) (1138,) (285,)\n",
      "Train on 1138 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1138/1138 [==============================] - 9s 8ms/sample - loss: 1747782.3504 - val_loss: 395306.5282\n",
      "Epoch 2/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 397364.3054 - val_loss: 311240.5305\n",
      "Epoch 3/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 310162.5612 - val_loss: 299658.1364\n",
      "Epoch 4/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 263947.3173 - val_loss: 289525.1939\n",
      "Epoch 5/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 261802.4543 - val_loss: 233215.6426\n",
      "Epoch 6/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 232395.1879 - val_loss: 231235.5667\n",
      "Epoch 7/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 233159.1720 - val_loss: 367312.4447\n",
      "Epoch 8/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 233657.3115 - val_loss: 246796.2480\n",
      "Epoch 9/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 215084.6163 - val_loss: 213457.2187\n",
      "Epoch 10/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 221374.8611 - val_loss: 207991.9060\n",
      "(1138, 4, 18) (285, 4, 18) (1138,) (285,)\n",
      "Train on 1138 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1138/1138 [==============================] - 7s 6ms/sample - loss: 1852988.7330 - val_loss: 626838.4187\n",
      "Epoch 2/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 479739.4839 - val_loss: 474495.6150\n",
      "Epoch 3/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 357231.5314 - val_loss: 359043.2389\n",
      "Epoch 4/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 316363.5167 - val_loss: 339309.9658\n",
      "Epoch 5/10\n",
      "1138/1138 [==============================] - 4s 3ms/sample - loss: 307032.7709 - val_loss: 353717.6052\n",
      "Epoch 6/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 301537.2600 - val_loss: 417244.5153\n",
      "Epoch 7/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 290156.9912 - val_loss: 446584.0692\n",
      "Epoch 8/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 282942.5917 - val_loss: 349637.4850\n",
      "Epoch 9/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 270353.5143 - val_loss: 315580.6643\n",
      "Epoch 10/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 275392.9086 - val_loss: 479940.9341\n",
      "(1138, 4, 18) (285, 4, 18) (1138,) (285,)\n",
      "Train on 1138 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1138/1138 [==============================] - 7s 6ms/sample - loss: 1870599.2156 - val_loss: 1856983.5781\n",
      "Epoch 2/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 547128.4170 - val_loss: 1211481.0259\n",
      "Epoch 3/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 393276.7990 - val_loss: 1143433.6362\n",
      "Epoch 4/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 343347.0389 - val_loss: 1130194.2950\n",
      "Epoch 5/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 333485.2998 - val_loss: 978684.2458\n",
      "Epoch 6/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 306064.2717 - val_loss: 972031.3722\n",
      "Epoch 7/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 304051.0503 - val_loss: 941525.3820\n",
      "Epoch 8/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 296633.9985 - val_loss: 935272.6439\n",
      "Epoch 9/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 275918.6037 - val_loss: 1198305.6775\n",
      "Epoch 10/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 267734.6094 - val_loss: 1181963.7922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|█████████████████████████████████████████████████████████████▋                   | 16/21 [30:41<09:42, 116.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1137, 4, 18) (285, 4, 18) (1137,) (285,)\n",
      "Train on 1137 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1137/1137 [==============================] - 8s 7ms/sample - loss: 10471862.0221 - val_loss: 13778755.0471\n",
      "Epoch 2/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 6038351.8171 - val_loss: 8528324.0864\n",
      "Epoch 3/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 4166294.6890 - val_loss: 6928456.6844\n",
      "Epoch 4/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 3953086.0639 - val_loss: 4867150.1823\n",
      "Epoch 5/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 3645274.2802 - val_loss: 4330805.2770\n",
      "Epoch 6/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 3535563.1233 - val_loss: 4783166.7127\n",
      "Epoch 7/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 3463028.7097 - val_loss: 4514821.2998\n",
      "Epoch 8/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 3461737.1006 - val_loss: 4194920.1661\n",
      "Epoch 9/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 3330459.9153 - val_loss: 4177744.7677\n",
      "Epoch 10/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 3274979.9871 - val_loss: 4771750.6552\n",
      "(1137, 4, 18) (285, 4, 18) (1137,) (285,)\n",
      "Train on 1137 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1137/1137 [==============================] - 7s 6ms/sample - loss: 8985491.5312 - val_loss: 7328482.0623\n",
      "Epoch 2/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 4616439.4511 - val_loss: 6327016.6835\n",
      "Epoch 3/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 4166024.1483 - val_loss: 5735976.8249\n",
      "Epoch 4/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 4134660.4839 - val_loss: 5715329.0576\n",
      "Epoch 5/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 3968408.9063 - val_loss: 5732802.7523\n",
      "Epoch 6/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 3938487.4627 - val_loss: 5744868.7589\n",
      "Epoch 7/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 3894983.8464 - val_loss: 6769829.3427\n",
      "Epoch 8/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 3700810.6883 - val_loss: 6173552.1390\n",
      "Epoch 9/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 3637115.6639 - val_loss: 7456244.7347\n",
      "Epoch 10/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 3638939.6591 - val_loss: 6048080.3976\n",
      "(1137, 4, 18) (285, 4, 18) (1137,) (285,)\n",
      "Train on 1137 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1137/1137 [==============================] - 7s 6ms/sample - loss: 13074694.0487 - val_loss: 11935930.8220\n",
      "Epoch 2/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 7364391.4483 - val_loss: 6271311.2517\n",
      "Epoch 3/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 5482668.2781 - val_loss: 5978801.9763\n",
      "Epoch 4/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 4896712.5292 - val_loss: 5445003.9423\n",
      "Epoch 5/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 4559069.4743 - val_loss: 6077003.0562\n",
      "Epoch 6/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 4457317.4738 - val_loss: 6312681.3451\n",
      "Epoch 7/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 4349505.1206 - val_loss: 6556767.7999\n",
      "Epoch 8/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 4242990.5280 - val_loss: 6355933.9947\n",
      "Epoch 9/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 4116582.7732 - val_loss: 6698770.2743\n",
      "Epoch 10/10\n",
      "1137/1137 [==============================] - 3s 3ms/sample - loss: 4077352.4843 - val_loss: 6113662.6492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|█████████████████████████████████████████████████████████████████▌               | 17/21 [32:39<07:48, 117.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1134, 4, 18) (284, 4, 18) (1134,) (284,)\n",
      "Train on 1134 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1134/1134 [==============================] - 6s 5ms/sample - loss: 1256126.4495 - val_loss: 414866.2991\n",
      "Epoch 2/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 294031.1970 - val_loss: 322499.4116\n",
      "Epoch 3/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 236175.0147 - val_loss: 394859.9519\n",
      "Epoch 4/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 225338.5483 - val_loss: 555494.7945\n",
      "Epoch 5/10\n",
      "1134/1134 [==============================] - 3s 2ms/sample - loss: 219409.1134 - val_loss: 452441.0330\n",
      "Epoch 6/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 225171.7143 - val_loss: 440286.5390\n",
      "Epoch 7/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 215008.2155 - val_loss: 499138.3098\n",
      "Epoch 8/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 206812.7689 - val_loss: 331253.2880\n",
      "Epoch 9/10\n",
      "1134/1134 [==============================] - 3s 2ms/sample - loss: 207056.3861 - val_loss: 590001.2103\n",
      "Epoch 10/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 204565.3056 - val_loss: 357261.0578\n",
      "(1134, 4, 18) (284, 4, 18) (1134,) (284,)\n",
      "Train on 1134 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1134/1134 [==============================] - 6s 5ms/sample - loss: 1401293.1623 - val_loss: 657842.8585\n",
      "Epoch 2/10\n",
      "1134/1134 [==============================] - 3s 2ms/sample - loss: 309830.4661 - val_loss: 455854.6762\n",
      "Epoch 3/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 253377.8577 - val_loss: 568683.8800\n",
      "Epoch 4/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 233760.3855 - val_loss: 557950.6660\n",
      "Epoch 5/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 221607.5785 - val_loss: 712639.7452\n",
      "Epoch 6/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 226427.6492 - val_loss: 562305.6899\n",
      "Epoch 7/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 220305.7253 - val_loss: 417962.1687\n",
      "Epoch 8/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 213182.8082 - val_loss: 441979.4145\n",
      "Epoch 9/10\n",
      "1134/1134 [==============================] - 3s 2ms/sample - loss: 212449.0726 - val_loss: 528340.4982\n",
      "Epoch 10/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 203290.4406 - val_loss: 364825.5805\n",
      "(1134, 4, 18) (284, 4, 18) (1134,) (284,)\n",
      "Train on 1134 samples, validate on 284 samples\n",
      "Epoch 1/10\n",
      "1134/1134 [==============================] - 6s 5ms/sample - loss: 1283062.5479 - val_loss: 980366.4958\n",
      "Epoch 2/10\n",
      "1134/1134 [==============================] - 3s 2ms/sample - loss: 335768.9608 - val_loss: 839655.3742\n",
      "Epoch 3/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 287310.6666 - val_loss: 730785.2616\n",
      "Epoch 4/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 277294.3457 - val_loss: 866226.8558\n",
      "Epoch 5/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 264316.6097 - val_loss: 674975.6508\n",
      "Epoch 6/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 262461.1463 - val_loss: 791694.4282\n",
      "Epoch 7/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 263011.9259 - val_loss: 789342.0496\n",
      "Epoch 8/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 247030.5236 - val_loss: 684134.6117\n",
      "Epoch 9/10\n",
      "1134/1134 [==============================] - 3s 2ms/sample - loss: 250766.1493 - val_loss: 715191.2102\n",
      "Epoch 10/10\n",
      "1134/1134 [==============================] - 3s 3ms/sample - loss: 236141.6613 - val_loss: 487912.9347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|█████████████████████████████████████████████████████████████████████▍           | 18/21 [34:21<05:37, 112.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1138, 4, 18) (285, 4, 18) (1138,) (285,)\n",
      "Train on 1138 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1138/1138 [==============================] - 7s 6ms/sample - loss: 1555547.4247 - val_loss: 757028.5694\n",
      "Epoch 2/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 374116.8075 - val_loss: 531552.9655\n",
      "Epoch 3/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 318830.4185 - val_loss: 473235.7436\n",
      "Epoch 4/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 312664.6202 - val_loss: 543425.7995\n",
      "Epoch 5/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 303887.2155 - val_loss: 541369.5702\n",
      "Epoch 6/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 305371.3563 - val_loss: 536229.2101\n",
      "Epoch 7/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 281417.6696 - val_loss: 500582.3673\n",
      "Epoch 8/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 298566.2566 - val_loss: 712176.0354\n",
      "Epoch 9/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 286834.1039 - val_loss: 927690.9848\n",
      "Epoch 10/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 264535.9042 - val_loss: 675522.9989\n",
      "(1138, 4, 18) (285, 4, 18) (1138,) (285,)\n",
      "Train on 1138 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1138/1138 [==============================] - 7s 6ms/sample - loss: 1481090.1599 - val_loss: 858657.0502\n",
      "Epoch 2/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 370893.4933 - val_loss: 636938.6514\n",
      "Epoch 3/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 351042.6894 - val_loss: 655994.3955\n",
      "Epoch 4/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 342059.2906 - val_loss: 600045.4495\n",
      "Epoch 5/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 333848.8102 - val_loss: 726610.9651\n",
      "Epoch 6/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 347401.2437 - val_loss: 762301.5753\n",
      "Epoch 7/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 327300.3225 - val_loss: 823042.5115\n",
      "Epoch 8/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 320318.3401 - val_loss: 945757.8071\n",
      "Epoch 9/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 322596.6918 - val_loss: 1052375.9340\n",
      "Epoch 10/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 320014.5413 - val_loss: 915126.7163\n",
      "(1138, 4, 18) (285, 4, 18) (1138,) (285,)\n",
      "Train on 1138 samples, validate on 285 samples\n",
      "Epoch 1/10\n",
      "1138/1138 [==============================] - 7s 6ms/sample - loss: 1404963.9764 - val_loss: 1059198.8406\n",
      "Epoch 2/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 422816.7271 - val_loss: 957493.4076\n",
      "Epoch 3/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 409485.3822 - val_loss: 1033264.3033\n",
      "Epoch 4/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 394982.2821 - val_loss: 1138058.9065\n",
      "Epoch 5/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 388777.7836 - val_loss: 962466.0419\n",
      "Epoch 6/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 387320.6230 - val_loss: 1099441.6110\n",
      "Epoch 7/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 386908.5756 - val_loss: 931130.4064\n",
      "Epoch 8/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 395284.1945 - val_loss: 952000.7059\n",
      "Epoch 9/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 360282.8052 - val_loss: 1054478.3843\n",
      "Epoch 10/10\n",
      "1138/1138 [==============================] - 3s 3ms/sample - loss: 362688.4261 - val_loss: 915040.6432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████████████████████████▎       | 19/21 [36:17<03:47, 113.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100, 4, 18) (276, 4, 18) (1100,) (276,)\n",
      "Train on 1100 samples, validate on 276 samples\n",
      "Epoch 1/10\n",
      "1100/1100 [==============================] - 9s 8ms/sample - loss: 6782402.0842 - val_loss: 3254787.9760\n",
      "Epoch 2/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 2173098.4429 - val_loss: 2706155.0969\n",
      "Epoch 3/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1795694.0185 - val_loss: 2751492.2142\n",
      "Epoch 4/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1720637.6450 - val_loss: 3037142.6723\n",
      "Epoch 5/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1710758.4074 - val_loss: 2165374.4064\n",
      "Epoch 6/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1579196.7886 - val_loss: 2504054.4298\n",
      "Epoch 7/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1618237.6757 - val_loss: 2324042.9514\n",
      "Epoch 8/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1566603.4837 - val_loss: 2142875.0296\n",
      "Epoch 9/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1545567.6473 - val_loss: 2090195.0301\n",
      "Epoch 10/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1570391.9268 - val_loss: 1996250.0063\n",
      "(1100, 4, 18) (276, 4, 18) (1100,) (276,)\n",
      "Train on 1100 samples, validate on 276 samples\n",
      "Epoch 1/10\n",
      "1100/1100 [==============================] - 7s 6ms/sample - loss: 7129627.2421 - val_loss: 4565407.6181\n",
      "Epoch 2/10\n",
      "1100/1100 [==============================] - 4s 3ms/sample - loss: 2283025.7337 - val_loss: 3026293.7201\n",
      "Epoch 3/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1894543.6016 - val_loss: 2576704.6757\n",
      "Epoch 4/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1886286.2285 - val_loss: 2978757.3394\n",
      "Epoch 5/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1717668.5949 - val_loss: 2533029.4360\n",
      "Epoch 6/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1696359.7175 - val_loss: 2473538.0058\n",
      "Epoch 7/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1698892.8767 - val_loss: 2180497.5624\n",
      "Epoch 8/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1661254.6595 - val_loss: 2240218.2087\n",
      "Epoch 9/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1725939.5422 - val_loss: 2960886.2175\n",
      "Epoch 10/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1683467.4518 - val_loss: 2120163.3160\n",
      "(1100, 4, 18) (276, 4, 18) (1100,) (276,)\n",
      "Train on 1100 samples, validate on 276 samples\n",
      "Epoch 1/10\n",
      "1100/1100 [==============================] - 8s 7ms/sample - loss: 6561087.2624 - val_loss: 4805397.1905\n",
      "Epoch 2/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 2321417.1495 - val_loss: 3351538.8301\n",
      "Epoch 3/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 2067889.0797 - val_loss: 3888962.2793\n",
      "Epoch 4/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1896835.3989 - val_loss: 2786097.0025\n",
      "Epoch 5/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1865999.9521 - val_loss: 3554953.3611\n",
      "Epoch 6/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1943621.1480 - val_loss: 2871280.6217\n",
      "Epoch 7/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1974306.6223 - val_loss: 2514070.8242\n",
      "Epoch 8/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1899070.0685 - val_loss: 2932948.0293\n",
      "Epoch 9/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1806237.2610 - val_loss: 2366772.6975\n",
      "Epoch 10/10\n",
      "1100/1100 [==============================] - 3s 3ms/sample - loss: 1727720.3128 - val_loss: 2195200.1221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████████▏   | 20/21 [38:13<01:54, 114.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(696, 4, 18) (174, 4, 18) (696,) (174,)\n",
      "Train on 696 samples, validate on 174 samples\n",
      "Epoch 1/10\n",
      "696/696 [==============================] - 6s 9ms/sample - loss: 107683577.3499 - val_loss: 91943035.7407\n",
      "Epoch 2/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 30217262.8326 - val_loss: 52593425.9598\n",
      "Epoch 3/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 19480806.2256 - val_loss: 28533793.4124\n",
      "Epoch 4/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 15325856.0620 - val_loss: 25605520.3513\n",
      "Epoch 5/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 13040579.4942 - val_loss: 26458458.9397\n",
      "Epoch 6/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 11984188.9438 - val_loss: 29273488.5162\n",
      "Epoch 7/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 11798473.2549 - val_loss: 29716620.6810\n",
      "Epoch 8/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 11143208.6820 - val_loss: 25304820.5297\n",
      "Epoch 9/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 11611267.2011 - val_loss: 26015770.5685\n",
      "Epoch 10/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 10695772.1491 - val_loss: 26458253.7202\n",
      "(696, 4, 18) (174, 4, 18) (696,) (174,)\n",
      "Train on 696 samples, validate on 174 samples\n",
      "Epoch 1/10\n",
      "696/696 [==============================] - 6s 8ms/sample - loss: 104178527.6451 - val_loss: 146626200.1839\n",
      "Epoch 2/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 38045879.9059 - val_loss: 62530620.2629\n",
      "Epoch 3/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 26776686.8797 - val_loss: 60626944.8966\n",
      "Epoch 4/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 19905051.6449 - val_loss: 26945780.6378\n",
      "Epoch 5/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 19331563.9955 - val_loss: 26477611.2552\n",
      "Epoch 6/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 16682135.6706 - val_loss: 26065843.2989\n",
      "Epoch 7/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 15423830.4957 - val_loss: 25646588.4292\n",
      "Epoch 8/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 15524186.5478 - val_loss: 30372079.7485\n",
      "Epoch 9/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 14235513.9751 - val_loss: 29407926.3200\n",
      "Epoch 10/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 15000015.0499 - val_loss: 30439597.8789\n",
      "(696, 4, 18) (174, 4, 18) (696,) (174,)\n",
      "Train on 696 samples, validate on 174 samples\n",
      "Epoch 1/10\n",
      "696/696 [==============================] - 6s 8ms/sample - loss: 117320148.8758 - val_loss: 133731692.8578\n",
      "Epoch 2/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 51900730.4725 - val_loss: 91636319.2823\n",
      "Epoch 3/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 39222377.0147 - val_loss: 80651789.4899\n",
      "Epoch 4/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 30090486.8958 - val_loss: 42724391.3685\n",
      "Epoch 5/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 22826808.5265 - val_loss: 36657406.3324\n",
      "Epoch 6/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 21975367.4982 - val_loss: 42940704.2040\n",
      "Epoch 7/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 20669092.3290 - val_loss: 51963734.7601\n",
      "Epoch 8/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 18978345.5043 - val_loss: 53838103.3189\n",
      "Epoch 9/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 16183438.4912 - val_loss: 65847190.7543\n",
      "Epoch 10/10\n",
      "696/696 [==============================] - 2s 3ms/sample - loss: 16609387.9061 - val_loss: 74597204.7827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 21/21 [39:28<00:00, 112.77s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model_dict = {}\n",
    "ts = 28\n",
    "data_dim = 18\n",
    "scaler_dict = {}\n",
    "\n",
    "for pum in tqdm(unique_pum + unique_kind):\n",
    "    # 품목 품종별 전처리\n",
    "    temp_df = train[['date',f'{pum}_거래량(kg)', f'{pum}_가격(원/kg)']]\n",
    "    temp_df = preprocessing(temp_df, pum).iloc[20:-28,:]\n",
    "    \n",
    "    # 주차별(1,2,4w) 학습\n",
    "    for week_num in [1,2,4] :\n",
    "        x = temp_df[temp_df[f'{week_num}_week']>0].iloc[:,:-3]\n",
    "        y = temp_df[temp_df[f'{week_num}_week']>0][f'{week_num}_week']\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaler = scaler.fit(x)       \n",
    "        scaler_dict[f'{pum}_model_{week_num}'] = X_scaler     \n",
    "        x = pd.DataFrame(X_scaler.transform(x))\n",
    "        \n",
    "        X, Y = make_dataset(x,y,4)\n",
    "        \n",
    "        #train, test split\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.2,shuffle = False, stratify = None)\n",
    "        print(x_train.shape, x_valid.shape, y_train.shape, y_valid.shape)\n",
    "        \n",
    "        model = tf.keras.models.Sequential([LSTM(128,\n",
    "                                                 input_shape=(X.shape[1], X.shape[2]),\n",
    "                                                 activation='relu', \n",
    "                                                 return_sequences=True),\n",
    "                                            Dense(128),\n",
    "                                            LSTM(128,\n",
    "                                                 input_shape=(X.shape[1], X.shape[2]),\n",
    "                                                 activation='relu'),\n",
    "                                            Dense(1)\n",
    "            ])\n",
    "        \n",
    "        model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "        model.fit(x_train,y_train, epochs = 10 , batch_size= 7, validation_data = (x_valid, y_valid))\n",
    "        model_dict[f'{pum}_model_{week_num}'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 38/38 [03:19<00:00,  5.24s/it]\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "public_date_list = submission[submission['예측대상일자'].str.contains('2020')]['예측대상일자'].str.split('+').str[0].unique()\n",
    "# ['2020-09-29', ...]\n",
    "\n",
    "for date in tqdm(public_date_list) :\n",
    "    test = pd.read_csv(f'./data/public_data/test_files/test_{date}.csv')\n",
    "    for pum in unique_pum + unique_kind:\n",
    "        # 예측기준일에 대해 전처리\n",
    "        temp_test = pd.DataFrame([{'date' : date}]) #예측기준일\n",
    "        alldata = pd.concat([train, test, temp_test], sort=False).reset_index(drop=True)\n",
    "        alldata = alldata[['date', f'{pum}_거래량(kg)', f'{pum}_가격(원/kg)']].fillna(0)\n",
    "        alldata = alldata.iloc[-28:].reset_index(drop=True)\n",
    "        alldata = preprocessing(alldata, pum)\n",
    "        temp_test = alldata\n",
    "        \n",
    "        x = temp_test.iloc[:,:-3]\n",
    "        y = temp_test.iloc[:,-3:]\n",
    "       \n",
    "       \n",
    "        # 개별 모델을 활용하여 1,2,4주 후 가격 예측\n",
    "        for week_num in [1,2,4] :\n",
    "            sc = scaler_dict[f'{pum}_model_{week_num}']\n",
    "            x = pd.DataFrame(sc.transform(x))\n",
    "            X, Y = make_dataset(x,y,4)\n",
    "            \n",
    "            temp_model = model_dict[f'{pum}_model_{week_num}']\n",
    "            result = temp_model.predict(X)\n",
    "            \n",
    "            condition = (submission['예측대상일자']==f'{date}+{week_num}week')\n",
    "            idx = submission[condition].index\n",
    "            submission.loc[idx, f'{pum}_가격(원/kg)'] = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('baseline_1028.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
